<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Weymire</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.weymire.com/"/>
  <updated>2017-05-28T07:13:36.000Z</updated>
  <id>http://www.weymire.com/</id>
  
  <author>
    <name>Jingping Yu</name>
    <email>Mistral@foxmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>游戏特效第五期 - 几种科幻风格的着色器特效</title>
    <link href="http://www.weymire.com/2017/03/12/essay5/"/>
    <id>http://www.weymire.com/2017/03/12/essay5/</id>
    <published>2017-03-12T13:12:12.000Z</published>
    <updated>2017-05-28T07:13:36.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><p>本文同时也发表在我的<a href="https://zhuanlan.zhihu.com/p/25725870" target="_blank" rel="external">知乎专栏 - 大萌喵的Shader相册</a>上😄.</p>
<blockquote>
<p>作者：音速键盘猫</p>
<p>链接：https://zhuanlan.zhihu.com/p/25725870</p>
<p>来源：知乎</p>
<p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
</blockquote>
<h2>写在前面</h2>
<p>最近闲来无事, 因此给自己开了个新坑 - 为一款科幻风格的塔防游戏Hacker F-301(骇客F-301)编写着色器特效. 在这个过程中有了一些心得体会, 因此打算写出来分享. 希望大家多提宝贵意见.</p>
<h2>Topics</h2>
<p>本文将介绍如下几种特效:</p>
<blockquote>
<ol>
<li>Inking (模型描边)</li>
<li>Hologram (模型的全息图)</li>
<li>See - Through (渲染出物体被遮挡的部分, 类似于穿墙透视效果 - 屏幕后期特效)</li>
<li>Force Field (力场护盾效果)</li>
<li>Video Glitch (模拟LCD显示屏受到电子干扰的效果 - 屏幕后期特效)</li>
</ol>
</blockquote>
<h2>Inking (模型描边, Outline)</h2>
<h4>何为Inking</h4>
<p>Inking是附加在<strong>蒙皮网格</strong>上的模型特效, 它用比较<strong>细</strong>的<strong>灰黑色</strong>的线条<strong>勾勒出网格的轮廓</strong>. 这样做的好处是能够从背景更加<strong>清晰</strong>地勾画出这个网格, 尤其是在对比度比较低的区域中. Inking特效的应用场景特别多, 大家耳熟能详的LOL中就出现了它的踪影:</p>
<p><img src="/image/essay5/1.jpg" alt="">
<img src="/image/essay5/2.jpg" alt=""></p>
<p>上面的两张图来源于一篇对LOL渲染流程分析的博客[1]. 第一张图中是原图, 而第二张图是加入Inking特效后的结果. 我们看到, 加入Inking后, 所有的模型能够更容易地从背景中区分出来, 起到了<strong>Bump Up</strong>的作用. 这个例子中使用了比较粗的Inking线条, 这样也增添了一分<strong>漫画风格</strong>的质感.</p>
<h4>Inking的实现方法(综述)</h4>
<p>Inking的实现方法有很多种, 大体上可以分为操作点元和操作片元两大类. 视具体情况决定使用哪一种Inking:</p>
<ul>
<li>Fresnel(菲尼尔)方法 - 非常类似于Rim Lighting, 使用视线方向和点法线方向的点积来判断边缘, 并将边缘高亮化.</li>
</ul>
<blockquote>
<p>优点: 效率高; 不需要单独的Pass就可以实现; 几乎所有的平滑的边缘都会得到高亮效果; 甚至对透明和半透明物体也有效.</p>
<p>缺点: 无法控制Inking线条的粗细, 这是因为Fresnel方法是针对于模型法线和摄像机视线的, 从而导致其仅与每个表面的法线方向有关, 而与表面的深度信息无关.</p>
</blockquote>
<p><img src="/image/essay5/3.jpg" alt=""></p>
<ul>
<li>Mesh Doubling (复制网格) - 非常类似于卡通Toon特效. 需要一个单独的Pass来实现. 重新绘制一个将所有表面都沿着法线方向延展过的模型, 然后将正面剪裁掉. <strong>这也是我采用的方案</strong></li>
</ul>
<blockquote>
<p>优点: 效率高; 平台适应性好; 可以控制Inking的线条粗细.</p>
<p>缺点: 线条并不连续, 在平滑表面的表现虽然很好, 但是在锐利的表面上经常会出现断层; 只能绘制最外层轮廓, 而不对内部结构做任何处理.</p>
</blockquote>
<p><img src="/image/essay5/4.png" alt=""></p>
<ul>
<li>Edge Detection (边缘检测) - Unity自带的屏幕后期处理特效[2]. 使用Sobel Filter[3]进行描边的算法, 其基本原理是检测多个相邻的像素的深度差值, 使用一个3x3的采样块来对原图求卷积, 将深度信息差值比较大的部分过滤出来. LOL中的Inking使用的就是这个方法.</li>
</ul>
<blockquote>
<p>优点: 既可以用作屏幕后期特效, 又可以作为模型特效; 描边准确; 线条粗细可控.</p>
<p>缺点: 比上述两种方案都要昂贵得多, 但是其性能开销恒定, 与被处理的图像没有任何关系;</p>
</blockquote>
<p><img src="/image/essay5/6.jpg" alt=""></p>
<ul>
<li>使用几何着色器 - 检验临近的多边形以确定邻边和夹角, 再单独构建轮廓的几何体.</li>
</ul>
<blockquote>
<p>优点: 目前为止最为精确的做法; 很容易控制线条的粗细.</p>
<p>缺点: 建议买一台给力点的工作站或服务器; 一般只能用于离线渲染;</p>
</blockquote>
<h4>具体实现策略</h4>
<p>采用了Mesh Doubling (复制网格)的方法. 这里必须要解决的问题是<strong>线条的不连续性</strong>. 其思路是<strong>不严格地将表面沿着法线方向延展, 而是在标准化的点元位置和法线方向之间取一个恰当的参数来做插值</strong>, 这样做的好处是表面在延展的过程中也会尽量向点元方向靠拢, 尽量地减少了新网格的<strong>撕裂感</strong>.</p>
<p>$$
P_{new} = P_{old} + L \times W_{outline} / D_{cam}
$$</p>
<p>$$
L = Normalize(MV_{IT} \times lerp(V, N, f))
$$</p>
<p>其中, L表示偏移向量; W表示轮廓线条粗细; D是物体和摄像机间的距离. V是标准化后的顶点坐标, 表示方向; N是顶点向量; f是插值参数.</p>
<p><img src="/image/essay5/5.png" alt=""></p>
<p>上图更加清晰地阐述了<strong>撕裂</strong>和<strong>不连续</strong>的情况. 如果不进行插值, 那么这种方法可以适用于球形等表面变化均匀且光滑的几何体, 但是对于立方体则无能为力.</p>
<p><img src="/image/essay5/7.png" alt=""></p>
<p>上图中的立方体的延展向量使用了参数0.032作为插值, 撕裂感便不复存在了.</p>
<p>这里给出Inking特效的核心程序代码(非常简短), 同时附上部分Implementation Notes:</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line"><span class="function">vertexOutput <span class="title">vert</span> <span class="params">( appdata_base v )</span></span></div><div class="line">&#123;</div><div class="line">		vertexOutput o;</div><div class="line">          </div><div class="line">		o.pos = mul ( UNITY_MATRIX_MVP, v.vertex );</div><div class="line">		float3 dir = normalize ( v.vertex.xyz );</div><div class="line">		float3 dir2 = v.normal;</div><div class="line">		</div><div class="line">		dir = lerp ( dir, dir2, _Factor );</div><div class="line"></div><div class="line">		dir = mul ( ( float3x3 ) UNITY_MATRIX_IT_MV, dir );</div><div class="line">		float2 offset = TransformViewToProjection ( dir.xy );</div><div class="line">		offset = normalize ( offset );</div><div class="line">		<span class="keyword">float</span> dist = distance ( mul ( UNITY_MATRIX_M, v.vertex ), _WorldSpaceCameraPos );</div><div class="line">		o.pos.xy += offset * o.pos.z * _OutlineWidth / dist;</div><div class="line"> </div><div class="line">		<span class="keyword">return</span> o;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>_Factor即为插值参数. 变换法向量要注意使用Model View矩阵的<strong>转置逆矩阵</strong>. 为了保证最终的线条粗细维持世界坐标上的恒定, 而不随摄像机的移动发生改变, 因此延展的像素位置要除以摄像机距离. 最终片元着色器函数只需要一句<strong>return _Color;<strong>即可. 最后必须注意</strong>剪裁掉正面</strong>, 否则绘制出的不会是轮廓, 而是将模型包裹起来的保鲜膜😩 ...</p>
<p><img src="/image/essay5/9.png" alt=""></p>
<p><img src="/image/essay5/8.png" alt=""></p>
<p>上面两张图是加入特效的前后对比. 我们看到使用Inking后炮塔能够更加&quot;犀利&quot;地从背景中呈现出来.</p>
<p>一定程度上, <strong>Inking有点类似于SSAO</strong>. 两者都是尝试在几何体的交界处加入更深层次的阴影以让画面更有对比度. 关于SSAO我的<a href="https://zhuanlan.zhihu.com/p/25038820?refer=MeowShader" target="_blank" rel="external">知乎专栏第四篇</a>有过介绍.</p>
<h2>Hologram (模型的全息图)</h2>
<h4>何为Hologram</h4>
<p>全息图是一般以激光为光源, 将被摄物体记录为3D光场(Light Field)所构成的三维图像. 一般以干涉条纹的形式存在.</p>
<p><img src="/image/essay5/10.jpg" alt=""></p>
<p><img src="/image/essay5/11.jpg" alt=""></p>
<p>上图中第一张图是全息投影仪的概念效果, 第二张图是质量效应(Mass Effect)中的特效.</p>
<p>Hologram一般可以用作<strong>单位建造的预览效果</strong>和<strong>呈现结构的效果</strong>.</p>
<p><img src="/image/essay5/17.jpg" alt=""></p>
<p><img src="/image/essay5/18.jpg" alt=""></p>
<h4>第一次尝试(Naive 方法)</h4>
<p>计算模型的每个片元的屏幕坐标, 然后对一个条纹状纹理采样即可. 为了防止单一的条纹过于无聊, 同时还引入了一个Noise Map来进行干扰. 代码如下:</p>
<p><figure class="highlight"><table><tr><td class="code"><pre><div class="line"></div><div class="line">v2f vert ( appdata_base v )</div><div class="line">&#123;</div><div class="line">    v2f o;</div><div class="line"></div><div class="line">    o.pos = UnityObjectToClipPos ( v.vertex );</div><div class="line"></div><div class="line">    o.uv = v.texcoord.xy;</div><div class="line"></div><div class="line">    o.screenPos = ComputeScreenPos ( o.pos );</div><div class="line"></div><div class="line">    o.dist = distance ( mul ( UNITY_MATRIX_M, float4 ( 0.0, 0.0, 0.0, 0.0 ) ), _WorldSpaceCameraPos );</div><div class="line"></div><div class="line">    return o;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">fixed4 frag ( v2f i ) : COLOR</div><div class="line">&#123;</div><div class="line">    fixed4 finalColor;</div><div class="line"></div><div class="line">    float2 uvNormal = UnpackNormal ( tex2D ( _NormalTex, i.uv ) ) / i.dist;</div><div class="line"></div><div class="line">    float2 screenUV = ( i.screenPos.xy / i.screenPos.w + float2 ( _TilingX * _Time.y, _TilingY * _Time.y ) ) * i.dist * _Distance;</div><div class="line"></div><div class="line">    fixed3 color = _Color * tex2D ( _MainTex, screenUV + uvNormal ) * _Emission;</div><div class="line"></div><div class="line">    fixed alpha = _Color.a * max ( min ( color.r, color.g ), color.b );</div><div class="line"></div><div class="line">    return fixed4 ( color, alpha );</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>得到的结果自然也是Naive的😅 :</p>
<p><img src="/image/essay5/12.gif" alt=""></p>
<p>对比上面的两张效果图, 我们发现一个问题: 虽然当前的这个Hologram特效能够显示出干涉条纹, 但是整个物体的深度和法线信息全部丢失, 给人以一种乱糟糟的线条感.</p>
<h4>第二次尝试(将深度和法线信息纳入考量)</h4>
<p>仔细观察前面的两张效果图, 我们看到dot(viewDirection, normalDirection)(以下简称为点积)越大则越昏暗. 为了让整个特效更加有层次感, 我选择的方案是分别计算点积大的区域和点积小的区域来的颜色信息. 同时我更新了对干涉条纹的计算方法: 为了防止整个全息图特效的颜色过淡, 先给予一个统一的强度_Strength, 然后再加上对干涉条纹的采样值.</p>
<p>$$
\alpha = F_{soft} \times (Z_{scene} - Z_{object})
$$</p>
<p>$$
S_{edge} = (1 - abs(N \cdot V)) ^ {I_{edge}} \times M_{edge}
$$</p>
<p>$$
S_{surf} = (abs(N \cdot V)) ^ {I_{surf}} \times M_{surf}
$$</p>
<p>$$
C_{hologram} = tex(T_{main}, wcoord + UV_{time})
$$</p>
<p>$$
C = \alpha * (S_{edge} + S_{surf}) * (C_{normal} + C_{hologram})
$$</p>
<p>得到的结果如下:</p>
<p><img src="/image/essay5/13.gif" alt=""></p>
<p>着色器代码如下:
<figure class="highlight"><table><tr><td class="code"><pre><div class="line"></div><div class="line">  v2f vert ( appdata_base v )</div><div class="line">    &#123;</div><div class="line">        v2f o;</div><div class="line"></div><div class="line">        o.pos = UnityObjectToClipPos ( v.vertex );</div><div class="line"></div><div class="line">        o.projPos = ComputeScreenPos ( o.pos );</div><div class="line"></div><div class="line">        o.uv = v.texcoord.xy;</div><div class="line"></div><div class="line">        o.normalDir = UnityObjectToWorldNormal ( v.normal );</div><div class="line"></div><div class="line">        o.posWorld = mul ( UNITY_MATRIX_M, v.vertex );</div><div class="line"></div><div class="line">        return o;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    fixed4 frag ( v2f i ) : COLOR</div><div class="line">    &#123;</div><div class="line">        fixed alpha = 1;</div><div class="line">        float sceneZ = LinearEyeDepth ( SAMPLE_DEPTH_TEXTURE_PROJ ( _CameraDepthTexture, UNITY_PROJ_COORD ( i.projPos ) ) );</div><div class="line">        float partZ = i.projPos.z;</div><div class="line">        float fade = saturate ( _InvFade * ( sceneZ - partZ ) );</div><div class="line">        alpha *= fade;</div><div class="line">                </div><div class="line">        float3 viewDirection = normalize ( _WorldSpaceCameraPos.xyz - i.posWorld.xyz );                </div><div class="line">                 </div><div class="line">        float4 objectOrigin = mul ( unity_ObjectToWorld, float4 ( 0.0, 0.0, 0.0, 1.0 ) );</div><div class="line"></div><div class="line">        float dist = distance ( _WorldSpaceCameraPos.xyz, objectOrigin.xyz );</div><div class="line"></div><div class="line">        float2 wcoord = i.projPos.xy / i.projPos.w;</div><div class="line">wcoord.x *= _Inter.y;</div><div class="line">        wcoord.y *= _Inter.z;</div><div class="line">        wcoord *= dist * _Inter.x;</div><div class="line">        </div><div class="line">        float3 nMask = _Strength;</div><div class="line">                 </div><div class="line">        float3 hMask = tex2D( _MainTex, wcoord + float2 ( 0, _Time.x * _Inter.w ) );</div><div class="line"></div><div class="line">        float fresnel = pow ( abs ( dot ( viewDirection, i.normalDir ) ), _FresPow ) * _FresMult;</div><div class="line">        float3 bLayer = lerp ( _bLayerColorA, _bLayerColorB, fresnel );</div><div class="line"></div><div class="line">        float fresnelOut = pow ( 1 - abs ( dot ( viewDirection, i.normalDir ) ), _FresPowOut ) * _FresMultOut;</div><div class="line">        float3 bLayerC = _bLayerColorC * fresnelOut;</div><div class="line"></div><div class="line">        float3 final = saturate ( ( hMask + nMask ) * ( bLayer + bLayerC ) ) * alpha;</div><div class="line">            </div><div class="line">        return float4 ( final * _Fade, 1) ;</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<h2>See - Through(透视特效)</h2>
<h4>何为透视特效</h4>
<p><img src="/image/essay5/14.jpg" alt=""></p>
<p>游戏中总有一些<strong>非常重要的物体</strong>, 需要确保玩家在任何时候都能以某种方式看到. 比如Hitman系列中玩家可以使用这种透视的方式来知道敌人的位置以确定自己的战术. 而RTS类游戏(比如红色警戒3)中被遮挡的单位也会以另一种颜色被渲染出来, 防止玩家不知道其存在.</p>
<h4>如何实现透视特效</h4>
<p>将游戏物体分为两层: Occluder(遮挡)层和Behind(后面)层. 特效要实现的目标是将Behind层被Occluder层遮挡的部分渲染出来. 这里使用两个摄像机, 分别渲染两个层的深度信息, 得到两张Render Target(以下简称RT). 将所有Behind RT深度大于对应Occluder RT深度的部分以另一种方式渲染出来, 而不对其余部分做任何处理, 并将结果放到一个新的RT中. 最终画一个全屏幕的Quad, 将这个RT直接Apply即可.</p>
<p>为了增加渲染结果的层次感并反应被遮挡物体的结构, 渲染Behind层的摄像机可以同时渲染法线信息, 然后在渲染最终RT的时候将颜色强度与法线方向挂钩即可.</p>
<p>以下是渲染结果:</p>
<p><img src="/image/essay5/15.gif" alt=""></p>
<p>如何让Rendering Path为Forward的摄像机得到场景的深度和法线信息呢? 使用单独的着色器规定其渲染行为, 然后使用Camera.RenderWithShader即可.</p>
<p>规定摄像机渲染方式的着色器:</p>
<p><figure class="highlight"><table><tr><td class="code"><pre><div class="line"></div><div class="line">struct v2f </div><div class="line">&#123;</div><div class="line">	float4 pos : POSITION;</div><div class="line">	float4 nz : TEXCOORD0;</div><div class="line">&#125;;</div><div class="line">            </div><div class="line">v2f vert( appdata_base v )</div><div class="line">&#123;</div><div class="line">	v2f o;</div><div class="line"></div><div class="line">	o.pos = mul( UNITY_MATRIX_MVP, v.vertex );</div><div class="line"></div><div class="line">	o.nz.xyz = COMPUTE_VIEW_NORMAL;</div><div class="line">	o.nz.w = COMPUTE_DEPTH_01;</div><div class="line"></div><div class="line">	return o;</div><div class="line">&#125;</div><div class="line">            </div><div class="line">fixed4 frag( v2f i ) : COLOR </div><div class="line">&#123;</div><div class="line">	return EncodeDepthNormal ( i.nz.w, i.nz.xyz );</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>处理两张RT的着色器:</p>
<p><figure class="highlight"><table><tr><td class="code"><pre><div class="line"></div><div class="line">v2f vert ( appdata_img v )</div><div class="line">&#123;</div><div class="line">	v2f o;</div><div class="line"></div><div class="line">	o.pos = UnityObjectToClipPos ( v.vertex );</div><div class="line"></div><div class="line">	o.uv = v.texcoord.xy;</div><div class="line"></div><div class="line">	return o;</div><div class="line">&#125;</div><div class="line"></div><div class="line">fixed4 frag ( v2f i ) : COLOR</div><div class="line">&#123;</div><div class="line">	float behindDepth, occluderDepth;</div><div class="line">	float3 behindNormal, occluderNormal;</div><div class="line"></div><div class="line">	DecodeDepthNormal ( tex2D ( _Behind, i.uv ), behindDepth, behindNormal );</div><div class="line">	DecodeDepthNormal ( tex2D ( _Occluder, i.uv ), occluderDepth, occluderNormal );</div><div class="line"></div><div class="line">	fixed4 scene = tex2D ( _MainTex, i.uv );</div><div class="line">	fixed4 pattern = tex2D ( _PatternTex, ( i.uv + _SinTime.w / 100 ) / _PatternScale );</div><div class="line"></div><div class="line">	if (behindDepth &gt; 0 &amp;&amp; occluderDepth &gt; 0 &amp;&amp; behindDepth &gt; occluderDepth)</div><div class="line">	&#123;</div><div class="line">		float factor = 0.1 + 0.9 * pow ( max ( dot ( float3 ( 0, 0, 1 ), behindNormal ), 0.0 ), 1.2 );</div><div class="line">		return fixed4 ( lerp ( scene, _Color, lerp ( factor, factor * pattern.r, _PatternWeight ) ) );</div><div class="line">	&#125;</div><div class="line"> 	else</div><div class="line">		return scene;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h4>一个小问题</h4>
<p>这个实现的方式开销有点大: 对于每一组Occluder层和Behind层, 都需要用两个摄像机单独渲染一次(虽然只需要深度和法线, 不需要计算光照), 然后做一次全屏幕的后期特效. 那么对于像红色警戒3这样的游戏, 总共有6个阵营的单位, 而每个阵营的单位被遮挡后都要被渲染为<strong>不同的颜色</strong>. 如果采用我的方法的话, 那就相当于要<strong>用7个(6 * Behind + 1 * Occluder)摄像机加6层后期特效</strong>, 显然在性能上是不可能的.</p>
<p>根据我的猜测, 红色警戒3应该是为每个单位额外附加了一个材质, 这个材质只有ZTest Fail的时候才会被渲染. 不知道这个猜测是否正确, 希望能与大家交流.</p>
<h2>ForceField (护盾特效)</h2>
<h4>何为ForceField</h4>
<p><img src="/image/essay5/16.jpg" alt=""></p>
<p>上图为TitanFall 2的截图. 图中泰坦的周围有一层半球形力场护盾. 如果有子弹打击到护盾上, 则对应被击中位置还会产生纹理上的变化, 并扭曲背景.</p>
<p>我们注意到这个ForceField与其他物体相交的位置都有高亮的特效. 关于相交高亮(Intersection Highlight)我的<a href="https://zhuanlan.zhihu.com/p/23971284?refer=MeowShader" target="_blank" rel="external">专栏第二篇文章</a>有过介绍.</p>
<h4>如何实现ForceField</h4>
<p>实现护盾本身并不难 --- 只需要一个半透明的材质就足够了. 难的是如果护盾被击中的话, 需要在<strong>被击中的位置产生变化</strong>.</p>
<p>为了解决这个问题, 我们必须将<strong>碰撞点的位置</strong>从世界坐标转换到模型坐标. 然后根据每个点元和碰撞点的距离来决定动态纹理的Alpha值即可.</p>
<p>$$
\alpha = A_{init} + \sum _{i = 1} ^{n} \frac {Dist_{i}} {Dist_{max}} \times B
$$</p>
<p>为了能够支持多个碰撞点, 可以在着色器中使用n个4*4矩阵以代表4 * n个碰撞点. 每个片元遍历一次所有的碰撞点信息即可. 护盾被击中以后, 其动态纹理的影响会随着时间逐渐减少, 因此动态纹理的Alpha也需要随着时间减弱(见下图).</p>
<p><img src="/image/essay5/19.gif" alt=""></p>
<p>时间可以通过脚本或着色器来控制. 只需要注意时间非负即可.</p>
<p>以下是着色器代码:</p>
<p><figure class="highlight"><table><tr><td class="code"><pre><div class="line"></div><div class="line">v2f vert ( appdata_base v )</div><div class="line">    &#123;</div><div class="line">        v2f o;</div><div class="line"></div><div class="line">        v.vertex += float4 ( v.normal * _MeshOffset, 0.0 );</div><div class="line"></div><div class="line">        o.pos = UnityObjectToClipPos ( v.vertex );</div><div class="line"></div><div class="line">        o.uv = v.texcoord.xy;</div><div class="line"></div><div class="line">        float3 worldPosition = mul ( UNITY_MATRIX_M, v.vertex );</div><div class="line"></div><div class="line">        float3 viewDirection = normalize ( worldPosition - _WorldSpaceCameraPos );</div><div class="line"></div><div class="line">        o.factor =  ( dot ( UnityObjectToWorldNormal ( v.normal ), viewDirection ) );</div><div class="line"></div><div class="line">        for ( int ii = 0; ii &lt; 4; ii++ )</div><div class="line">        &#123;</div><div class="line">            o.dist[ ii ] = distance ( _CollisionPoints[ ii ].xyz, v.vertex.xyz );</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        return o;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    fixed4 frag ( v2f i ) : COLOR</div><div class="line">    &#123;</div><div class="line">        fixed4 finalColor;</div><div class="line"></div><div class="line">        float2 uvNormal = UnpackNormal ( tex2D ( _NormalTex, i.uv * _NormalScale + float2 ( _TilingX * _Time.y, _TilingY * _Time.y ) ) );</div><div class="line"></div><div class="line">        fixed3 color = tex2D ( _MainTex, ( i.uv ) * _MainScale + uvNormal ) * _Color * _Emission;</div><div class="line"></div><div class="line">        float fallOff = saturate ( pow ( 1.0 - i.factor, _FallOff ) * pow ( i.factor, _FallOff2 ) );</div><div class="line"></div><div class="line">        ///Magic Number! </div><div class="line">        half alpha = 0.01;</div><div class="line"></div><div class="line">        alpha += saturate ( pow ( _CollisionTime.x, 0.5 ) - ( float ( i.dist[ 0 ] ) / _MaxDistance ) ) * _BrightnessCollision * max ( sign ( _CollisionTime.x ), 0.0 );</div><div class="line">        alpha += saturate ( pow ( _CollisionTime.y, 0.5 ) - ( float ( i.dist[ 1 ] ) / _MaxDistance ) ) * _BrightnessCollision * max ( sign ( _CollisionTime.y ), 0.0 );</div><div class="line">        alpha += saturate ( pow ( _CollisionTime.z, 0.5 ) - ( float ( i.dist[ 2 ] ) / _MaxDistance ) ) * _BrightnessCollision * max ( sign ( _CollisionTime.z ), 0.0 );</div><div class="line">        alpha += saturate ( pow ( _CollisionTime.w, 0.5 ) - ( float ( i.dist[ 3 ] ) / _MaxDistance ) ) * _BrightnessCollision * max ( sign ( _CollisionTime.w ), 0.0 );</div><div class="line"></div><div class="line">        finalColor.rgb = color;</div><div class="line">        finalColor.a = alpha * pow ( finalColor.b, 2.0 );</div><div class="line"></div><div class="line">        return finalColor;</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<p>以下是用于计算碰撞点模型空间坐标并为着色器赋值的脚本:</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"></div><div class="line">public void ShieldOnWorldSpacePoint (Vector3 point)</div><div class="line">&#123;</div><div class="line">	Vector3 localPosition = transform.InverseTransformPoint (point);</div><div class="line">	Vector4 toShield = new Vector4 (localPosition.x, localPosition.y, localPosition.z, 1f);</div><div class="line">	effectTime[currentIndex] = duration;</div><div class="line">	collisionPoints.SetRow(currentIndex, toShield);</div><div class="line">	material.SetMatrix(&quot;_CollisionPoints&quot;, collisionPoints);</div><div class="line">	material.SetVector(&quot;_CollisionTime&quot;, effectTime);</div><div class="line">	currentIndex++;</div><div class="line">	currentIndex %= 4;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>我这里总共使用了一个4*4的矩阵, 因此可以用来表示4个碰撞点. 这实际上已经非常足够了, 因为我们可以将每个碰撞的持续时间缩短, 做到及时更新. 根据我的观察, TitanFall2中的Doom Shield应该也只使用了最多8个碰撞点. 我曾经用Legion(就是拿重机枪的那个, 一秒钟能打十几发子弹)泰坦扫射对面的Doom Shield, 感觉最多也只有6, 7个碰撞点的样子.</p>
<p>我这里的实现方案没有引入HeatWave, 也就是说护盾被击中后不会扭曲后面的背景. 这是因为在这款游戏中护盾是加在防御塔上的, 而防御塔占据整个屏幕的空间比较小, 因此并不需要将效果做得很精致; 而因为防御塔的数量可能比较多, 因此性能是更大的考量, 所以采用了直接Blend的方式. 如果想要扭曲背景的话, 则使用GrabPass[4], 然后为uv添加一层Bump Map过滤即可[5].</p>
<h2>Video Glitch (干扰特效)</h2>
<p>在TitanFall 2中, 游戏以让画面呈现电子干扰的方式来通知玩家泰坦正在承受强大的火力攻击.</p>
<p><img src="/image/essay5/20.png" alt=""></p>
<p>在Alienation中, 游戏主界面也会不时地发生类似LCD显示屏模拟信号受到干扰的效果. (此处无图, 因为我在PS4上的截图无论如何也导不出来)</p>
<p>这种干扰特效归根到底可以分为两部分:</p>
<blockquote>
<ol>
<li>画面的扭曲.</li>
<li>颜色漂移.</li>
</ol>
</blockquote>
<p>其实这两步的实现都非常简单: 画面的扭曲可以通过更改采样的UV坐标来实现, 而颜色漂移则可以用另一UV坐标采样单独的某个颜色通道, 然后作为输出的颜色中对应的通道.</p>
<p>代码如下:</p>
<p><figure class="highlight"><table><tr><td class="code"><pre><div class="line"></div><div class="line">fixed4 frag ( v2f i ) : COLOR</div><div class="line">    &#123;</div><div class="line">        float u = i.uv.x;</div><div class="line">        float v = i.uv.y;</div><div class="line"></div><div class="line">        float horizonNoise = tex2D ( _NoiseTex, float2 ( v, _Time.x ) ) * 2 - 1;</div><div class="line">        horizonNoise *= step ( _Threshold, abs ( horizonNoise ) ) * _Amount;</div><div class="line"></div><div class="line">        float shake = ( tex2D ( _NoiseTex, float2 ( _Time.x, 2 ) - 0.5 ) ) * _Shake;</div><div class="line"></div><div class="line">        float drift = sin ( v + _DriftTime ) * _DriftAmount;</div><div class="line"></div><div class="line">        half4 color = tex2D ( _MainTex, frac ( float2 ( u + jitter + shake, v ) ) );</div><div class="line">        half4 color1 = tex2D ( _MainTex, frac ( float2 ( u + jitter + shake + drift, v ) ) );</div><div class="line"></div><div class="line">        return half4 ( color.r, color.g, color1.b, 1 );</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<p>最终效果如下:</p>
<p><img src="/image/essay5/21.gif" alt=""></p>
<h2>后记</h2>
<p>以上就是我近几天做的特效. 代码我会尽快上传到Github上, 到时大家多提宝贵意见. 如果有需要完善的地方敬请Raise Issues.</p>
<p>PS : 大三学生党, 实习工作快到碗里来 ~</p>
<h2>参考</h2>
<p>[1] A Trip Down The LOL Graphics Pipleline (JAN 12, 2017), https://engineering.riotgames.com/news/trip-down-lol-graphics-pipeline Retrieved 19:24, March 12, 2017, from Riot Games Engineering.</p>
<p>[2] https://docs.unity3d.com/Manual/script-EdgeDetectEffect.html</p>
<p>[3] Sobel operator. (2017, March 5). In Wikipedia, The Free Encyclopedia. Retrieved 01:45, March 13, 2017, from https://en.wikipedia.org/w/index.php?title=Sobel_operator&amp;oldid=768707694</p>
<p>[4] https://docs.unity3d.com/Manual/SL-GrabPass.html</p>
<p>[5] https://blogs.unity3d.com/cn/2011/09/08/special-effects-with-depth-talk-at-siggraph/</p>
]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;/assets/js/DPlayer.min.js&quot;&gt; &lt;/script&gt;&lt;p&gt;本文同时也发表在我的&lt;a href=&quot;https://zhuanlan.zhihu.com/p/25725870&quot; target=&quot;_blank&quot; rel=&quot;external
    
    </summary>
    
    
      <category term="Unity" scheme="http://www.weymire.com/tags/Unity/"/>
    
      <category term="Computer Graphics" scheme="http://www.weymire.com/tags/Computer-Graphics/"/>
    
      <category term="Shader" scheme="http://www.weymire.com/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>SSAO及其优化</title>
    <link href="http://www.weymire.com/2017/01/31/essay1/"/>
    <id>http://www.weymire.com/2017/01/31/essay1/</id>
    <published>2017-01-30T20:40:12.000Z</published>
    <updated>2017-02-12T12:05:44.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><p>本文同时也发表在我的<a href="https://zhuanlan.zhihu.com/p/25038820?refer=MeowShader" target="_blank" rel="external">知乎专栏</a>上😄.</p>
<blockquote>
<p>作者：音速键盘猫</p>
<p>链接：https://zhuanlan.zhihu.com/p/25038820</p>
<p>来源：知乎</p>
<p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
</blockquote>
<h2>何为SSAO</h2>
<p>Screen Space Ambient Occlusion (以下简称SSAO), 屏幕空间环境光遮蔽. 在具体介绍SSAO之前, 本文先介绍更加广义的Ambient Occlusion (AO).</p>
<p>简单来说, Ambient Occlusion(以下简称&quot;AO&quot;)是一种基于全局照明中的环境光(Ambient Light)参数和环境几何信息来计算场景中任何一点的<strong>光照强度系数</strong>的算法. AO描述了表面上的任何一点所接受到的环境光被周围几何体所遮蔽的百分比, 因此使得渲染的结果更加富有层次感, 对比度更高.
<img src="/image/essay1/1.png" alt=""></p>
<p>图片来自Wiki. 因为老人的皱纹处对外界暴露的部分较少, 使用AO后被遮蔽的部分较多, 渲染后显得更加暗一些, 增加了皱纹的层次感和质感.
AO的计算公式如下:
$$
AO(p, n_{p}) = \frac {1} {\pi} \int _{\Omega}V(p, \omega)max( n_{p} * \omega, 0 )d\omega
$$</p>
<p>$n_{p}$代表$p$的法线, $\omega$代表点$p$切平面正方向的任意单位向量, $V(p, \omega)$是可见函数, 如果点$p$在$\omega$方向<strong>被遮挡则为1, 否则为0</strong>.</p>
<p>由此可见, 计算AO系数是一个颇为昂贵的操作. 一般离线渲染器都会采用Ray-Tracing(光线追踪)或是简化的Ray-Marching(所谓光线行进)算法, 模拟若干条射线以计算遮蔽百分比. 很明显这种方式不可能应用到实时图形渲染中. 尽管目前有一些实时计算AO的新技术, 但是其性能距离普及还有很长的路要走.</p>
<p><img src="/image/essay1/2.png" alt=""></p>
<p>上图为基于Ray-Tracing的AO计算模型. 红色的射线表示V = 1, 绿色的射线表示V = 0.</p>
<p>那么我们能否Trade Off, 用差一点的渲染结果来获得更高的运行效率呢? 答案是肯定的, 而且方法还远不止一种. 本文将重点放在SSAO上.</p>
<p>顾名思义, &quot;Screen Space&quot;意味着SSAO并不是场景的预处理, 而是屏幕后期处理. 其原理是在片元着色器中对于屏幕上的每个像素模拟若干个位置随机的采样点, 用被遮蔽的采样点数量百分比来近似表示光照强度系数.</p>
<h2>SSAO的实现</h2>
<p>SSAO的实现可分为三个步骤: 计算AO, 模糊/滤波, 与Color Buffer混合.</p>
<h3>计算AO</h3>
<p>计算AO的核心问题在于如何取采样点并判断这些采样点是否被遮蔽. 我们首先解决第一个问题. 在此我们使用一种指向法线方向的半球形采样块(Sample Kernel), 并在采样块中生成采样点. 距离原点越远的点, AO贡献越小. 采样块如下图所示:
<img src="/image/essay1/3.png" alt="">
那么我们转入第二个问题: 如何判断下图中的采样点遮蔽情况呢?
<img src="/image/essay1/4.jpg" alt="">
(涂黑的点处在几何体表面内部, 因此判定为被遮蔽)</p>
<p>一种方法是将采样点全部投影到View Plane上, 相当于获取采样点的UV坐标, 并同时获取Depth Buffer中该UV坐标处的深度值. 随后比较采样点的深度和场景中该点的深度. 如果采样点的深度更大, 说明其被场景遮蔽. 最终将所有采样点的AO贡献求和, 即是该点的AO值. 计算公式如下:</p>
<p>$$
AO_{n}(p) = \frac {1} {n} C(p, s_{i})
$$
$$
C(p, s_{i}) = V(p, s_{i})cos(s_{i}-p, n_{p})D(s_{i}-p)
$$
其中, 函数V前面已经介绍过. 函数D是一个[0, 1]之间的单调递减函数, 距离原点越近的采样点对AO的贡献越大. 一般使用指数函数.
<img src="/image/essay1/5.png" alt="">
上图为原图
<img src="/image/essay1/6.png" alt="">
上图为求得的AO值. 颜色越深代表AO越大.
以下是循环采样部分代码(ii为循环变量):
<figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line">half3 randomDirection = RandomSample[ ii ];</div><div class="line">float2 uv_offset = randomDirection.xy * scale;</div><div class="line"></div><div class="line"><span class="keyword">float</span> randomDepth = depth - ( randomDirection.z * _Radius );</div><div class="line"></div><div class="line"><span class="keyword">float</span> sampleDepth;</div><div class="line">float3 sampleNormal;</div><div class="line"></div><div class="line">DecodeDepthNormal ( tex2D ( _CameraDepthNormalsTexture, i.uv + uv_offset ), sampleDepth, sampleNormal );</div><div class="line"></div><div class="line">sampleDepth *= _ProjectionParams.z;</div><div class="line"></div><div class="line"><span class="keyword">float</span> diff =  saturate( randomDepth - sampleDepth );</div><div class="line"><span class="keyword">if</span> ( diff &gt; _ZDiff )</div><div class="line">occlusionAmount += <span class="built_in">pow</span> (<span class="number">1</span> - diff, _Attenuation);</div></pre></td></tr></table></figure></p>
<p>到此我们发现了一个问题: 上面求得的AO结果非常不理想. 图中有非常明显的条带状阴影, 给人的感觉像是在图上轻轻地抹了一层均匀的油漆. 产生这种现象的原因很简单 - 为了满足实时渲染的性能要求, 我们必须限制采样点的数目.</p>
<p>但是, 对于这种现象我们有一个Trick ---- 可以引入噪声, 将每个采样点以原点法线方向为旋转轴旋转随机的角度. 这样的新采样点会变得极其不规则, 更加离散化. 将低频的条纹转化成高频的噪声.</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line">half3 randomVector = tex2D ( _RandomTexture, i.uv_random ).xyz * <span class="number">2.0</span> - <span class="number">1.0</span>;</div><div class="line">half3 randomDirection = reflect ( RandomSample[ ii ], randomVector );</div></pre></td></tr></table></figure></p>
<p><img src="/image/essay1/7.png" alt="">
上图为引入随机噪声后的采样结果. 我们发现&quot;油漆&quot;变成了&quot;沙子&quot;.</p>
<h3>模糊/滤波</h3>
<blockquote>
<p>&quot;油漆&quot;好还是&quot;沙子&quot;好?
都不好!</p>
</blockquote>
<p>油漆&quot;显得平淡无奇, &quot;沙子&quot;让人眼花缭乱. 中国人讲究中庸之道, 也就是说 ---- 我们需要一个&quot;中频&quot;的AO!</p>
<p>在此介绍两种方法. 第一种方法是直接模糊. 比较常用的是高斯模糊. 关于高斯模糊的资料有很多, 本文不再赘述.</p>
<p>第二种方法在采样原理上和高斯模糊别无二致, 只是采样系数由静态变为动态: 原点与采样点的UV坐标距离, 法线和深度关系共同决定采样系数, 距离越远采样系数越小, 法线和深度的差距越大则采样系数也越大. 这样的模糊使得结果更加趋近于中频, 进一步减弱了闪烁(Flickering)的效果.</p>
<h3>与ColorBuffer混合</h3>
<p>一般加入Gamma Correction使得阴影更有层次感, 即最终结果为:
<figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line">tex2D ( _MainTex, i.uv ) * <span class="built_in">pow</span> ( ( <span class="number">1</span> - occlusion ), <span class="number">2.2</span> );</div></pre></td></tr></table></figure></p>
<p><img src="/image/essay1/8.png" alt=""></p>
<p>上图为SSAO处理后的最终结果.</p>
<h2>SSAO的问题与优化策略</h2>
<p>SSAO技术的基本原理已经介绍完了, 下面我们来谈谈SSAO可能遇到的问题, 以及相应的解决方案:</p>
<h3>1. 采样块的问题</h3>
<p>上文的SSAO实现方案其实是假定了使用Deferred Rendering, 深度和法线都可以非常容易得获取到, 因此我们的半球形采样块可以沿着顶点的法线方向摆放. 但是如果获取法线比较困难, 我们可以将半球形退化成球形, 这也正是2007年Crysis中SSAO的实现方案.</p>
<p><img src="/image/essay1/9.jpg" alt="">
如上图所示, 采样块的摆放与法线无关, 采样点遍布整个球形中. 这种方法的效果自然不如半球形采样块.</p>
<h3>2. 重复计算的问题</h3>
<p>SSAO最为耗时的操作是模拟多个采样点并计算其AO贡献值, 因此我们应该想办法避免重复计算, 尽量使用以前的结果. 这里可以使用Reverse Reprojection(反向二次投影), 保存上一帧的AO计算信息, 使得当前帧中相对上一帧没有变化的点可以利用旧的AO信息, 避免重复计算. 这种方式称为SSAO with Temporal Coherence(时间相干性), 简称为TSSAO. 具体的实现方式将在下文中进行阐述.</p>
<h3>3. 滤波与精确计算之间的矛盾</h3>
<p>这也是一个Trade Off ---- 反正运算结果都是要套用滤波来过滤掉噪点, 那么最开始计算的时候就可以想办法在保证质量不受太大影响的前提下, 尽量提升效率. 举一个例子:</p>
<blockquote>
<p>假设有两个面饼师傅, 第一个师傅的工作是揉出来5个直径为1.2cm的面球, 第二个师傅的工作是把这5个面球放在一起拍成一个面饼.
我们不讨论这种工作方式是否合理, 只是在此情况下第一个师傅确实不用对1.2cm吹毛求疵, 只要差不多就行了.</p>
</blockquote>
<p>明白了这个道理, 就会发现可以在计算AO的时候使用降采样, 这样能成平方倍地降低SSAO的时间复杂度. 关于降采样网上有很多资料, 我的专栏第一篇文章也对此有过介绍.</p>
<h2>TSSAO</h2>
<h3>1. Reverse Reprojection</h3>
<p>交替使用两张Render Texture, 一张代表当前帧, 另一张代表上一帧. 对于当前帧上任何一个Pixel都可以根据其UV坐标重建其世界坐标, 然后根据上一帧的View-Projection矩阵的逆矩阵来转化成上一帧的相应UV坐标. 如果两帧上对应的Pixel的Depth与世界坐标差距不大, 那么当前帧就可以利用上一帧对应Pixel的信息, 免去重复计算.</p>
<p>对于静态场景我们有如下公式:
$$
t_{old_{x^{'}, y^{'}, z^{'}}} = P_{old}V_{old}V_{new}^{-1}P_{new}^{-1}t_{new_{x, y, z}}
$$
t表示NDC坐标, P表示Projection矩阵, V表示View矩阵.</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> float4 <span class="title">UV2WorldPos</span><span class="params">(float2 uv, float4x4 iv)</span></span></div><div class="line">&#123;</div><div class="line"><span class="keyword">float</span> depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, uv);</div><div class="line">float2 invClipSize = float2(_CurP._11, _CurP._22);</div><div class="line">float2 spos = (uv * <span class="number">2</span> - <span class="number">1</span>) / invClipSize;</div><div class="line"><span class="keyword">float</span> depthEye = LinearEyeDepth(depth);</div><div class="line">    float3 vpos = float3(spos, <span class="number">-1</span>) * depthEye;</div><div class="line"><span class="keyword">return</span> mul(iv, float4(vpos, <span class="number">1.0</span>));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> float2 <span class="title">WorldPos2UV</span><span class="params">(float4 worldPos, float4x4 vp)</span></span></div><div class="line">&#123;</div><div class="line">float4 vpos = mul(vp, worldPos);</div><div class="line">float2 projOffset = float2(_CurP._13, _CurP._23);</div><div class="line">float2 screenPos = vpos.xy / vpos.w + projOffset;</div><div class="line"><span class="keyword">return</span> <span class="number">0.5</span> * screenPos + <span class="number">0.5</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> float2 <span class="title">GetMotionVector</span><span class="params">(float2 uv)</span></span></div><div class="line">&#123;</div><div class="line">float4 worldPos = UV2WorldPos(uv, _CurIV);</div><div class="line">float2 curUv = WorldPos2UV(worldPos, _CurVP);</div><div class="line">float2 preUv = WorldPos2UV(worldPos, _PrevVP);</div><div class="line"><span class="keyword">return</span>  curUv - preUv;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>但是对于动态场景上述公式不再成立. 因为SSAO要做到Scene Independent, 同时还必须记录前后两帧的关联, 因此在此我们可以用一个Render Texture单独记录每个像素在前一帧与当前帧的世界坐标的差. 具体的原因详见下文.</p>
<h3>2. 新的AO计算公式</h3>
<p>在TSSAO中我们使用两个Render Texture分别作为上一帧和当前帧的AO Buffer. 首先计算当前帧的AO贡献值:
$$
C_{t+1}(p)=\frac {1} {k} \sum _{i=n_{t}(p)+1} ^{n_{t}(p)+k}C(p, s_{i})
$$
$n_{t}(p)$表示的是之前已经计算过的采样点数量, k表示每一帧应该计算的采样点数量.
随后, 利用当前帧和上一帧的AO贡献值共同计算当前帧的AO值:
$$
AO _{t+1}(p) = \frac {n_{t}(p)AO_{t}(p_{old}) + kC_{t+1}(p)} {n_{t}(p)+k}
$$
最后, 更新$n_{t}(p)$的值
$$
n_{t+1}(p) = min(n_{t+1}(p)+k, n_{max})$$</p>
<p>在此说明一下为什么要设置n的上限. 这里主要有两个原因, 第一是如果之前的计算结果不老化, 当前帧的AO贡献值会越来越小, 算法的反应会越来越慢. 第二是反向二次投影本身是有误差的, 随着投影次数的增加误差会变得非常大, 因此必须限制被使用的结果数量, 适当舍弃掉过老的数据.</p>
<h3>3. 检测不合法像素</h3>
<p>很容易想到的一个判定条件是深度检测: 如果新旧两个像素的深度差距过大, 那么说明场景 已经改变, 当前像素的AO值已经不正确, 必须全部舍弃. 相对深度关系检测条件如下:
$$
|1 - \frac {d_{new}} {d_{old}}| &lt; \epsilon
$$
但是, SSAO考虑的不仅仅是当前点, 还有它周边的环境. 举一个例子: 在一个静态的地面上放置着一个动态的立方体. 这个立方体随着时间不规则运动. 地面与立方体地面棱边的外交界处的AO值自然明显高出地面上其他点的值, 但是立方体的移动会使得地面上相应区域的AO值不再有效 ---- 虽然地面没有在动, 地面上的点能够通过深度关系检测.</p>
<p><img src="/image/essay1/10.png" alt=""></p>
<p>这里我们用到了另一个Trick ---- 我们在计算当前帧AO的循环中可以同时做出以下判定:
$$
| | s_{i} - p | - |s_{i_{old}} - p_{old}|| &lt; \epsilon
$$
只要有一个采样点不满足上述条件, 则说明其对应原点的周边环境已经发生改变, 其AO值自然也应该重新计算. 对于$s_{i_{old}}$的计算方法也很简单: 根据$s_{i}$和在第一步中记录的世界坐标的差就可以直接得出了.</p>
<p>只要像素被判定为不合法, 则其$n_{t}(p)$会被重置为0, 即之前的所有AO运算结果全部舍弃.</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line"><span class="keyword">int</span> ww = w;</div><div class="line"></div><div class="line"><span class="keyword">if</span> ( <span class="built_in">abs</span> ( <span class="number">1</span> - texelDepth / preDepth ) &gt;= EPS )</div><div class="line">ww = <span class="number">0</span>;</div><div class="line"></div><div class="line"><span class="keyword">float</span> scale = _Parameters.x / depth;</div><div class="line"></div><div class="line"><span class="keyword">for</span> ( <span class="keyword">int</span> ii = <span class="number">0</span>; ii &lt; _SampleCount; ii++ )</div><div class="line">&#123;</div><div class="line">float3 randomDirection = RAND_SAMPLES [ w + ii ];</div><div class="line">randomDirection *= -sign ( dot ( texelNormal, randomDirection ) );</div><div class="line">randomDirection += texelNormal * <span class="number">0.3</span>;</div><div class="line"></div><div class="line">float2 uv_offset = randomDirection.xy * scale;</div><div class="line"></div><div class="line"><span class="keyword">float</span> randomDepth = texelDepth - randomDirection.z * _Parameters.x;</div><div class="line"></div><div class="line"><span class="keyword">float</span> sampleDepth;</div><div class="line">float3 sampleNormal;</div><div class="line"></div><div class="line">GetDepthNormal ( i.uv + uv_offset, sampleDepth, sampleNormal );</div><div class="line"></div><div class="line">sampleDepth *= _ProjectionParams.z;</div><div class="line"></div><div class="line"><span class="keyword">float</span> diff = randomDepth - sampleDepth;</div><div class="line"></div><div class="line"><span class="keyword">if</span> ( diff &gt; _Parameters.y )</div><div class="line">occlusion += saturate ( <span class="built_in">cos</span> ( dot ( randomDirection, texelNormal ) ) ) * <span class="built_in">pow</span> ( <span class="number">1</span> - diff, _Parameters.z );</div><div class="line"></div><div class="line"><span class="keyword">if</span> ( <span class="built_in">abs</span> ( randomDirection - <span class="built_in">abs</span> ( randomDirection + GetMotionVector ( i.uv + uv_offset ) - GetMotionVector ( i.uv ) - texelPosition ) ) &gt;= EPS )</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3>4.滤波</h3>
<p>可以在SSAO的滤波基础上加上收敛度的条件. 收敛度定义为:
$$
conv=min (n_{t}(p), n_{max}) / n_{max}
$$
收敛度越大, 说明当前像素越为&quot;安全&quot;, 随着时间的改变越小, 因此采样系数也越大.</p>
<h2>Selective Temporal Filtering (STF)</h2>
<p>这项技术应用在了BattleField3中. 首先它是基于TSSAO的, 不同的是其AO计算方式: 在当前帧的AO贡献值和历史数据中间做插值.</p>
<p>这样做带来的一个小问题就是&quot;老化&quot; ---- 旧的数据不能及时清理出去, 这就导致场景移动比较快的时候, AO Buffer会存在鬼影的问题.</p>
<p>但是一个更为严重的问题是&quot;闪烁&quot; ---- BattleField3的场景中有大量花草树木, 树叶的晃动使得大量像素被频繁检测为失效, 重新计算AO, 这与未失效的部分构成了鲜明的对比.</p>
<p>DICE的解决方案非常Trick: 他们发现存在鬼影的像素和存在闪烁的像素是互斥的. 因此他们想办法甄别这两种像素, 并对于可能产生鬼影的像素关掉Temporal Filtering. 因此这项技术被称为Selective Temporal Filtering.</p>
<p>具体的方法是检测连续性: 对于任何一个Pixel, 连续在x或y方向选择两个像素, 判断这三个像素的深度是否连续. 如果连续则可能产生鬼影, 否则可能产生闪烁.</p>
<p><img src="/image/essay1/11.png" alt="">
最后将所有闪烁的像素按照4x4放大, 圈定进行Temporal Filter的区域:
<img src="/image/essay1/12.png" alt="">
<img src="/image/essay1/13.png" alt=""></p>
<h2>后记</h2>
<p>SSAO原理并不复杂, 只是在实际应用场景中会有各种各样的Trick以应对个性化的需要. 文中主要讲解了SSAO的基本原理与TSSAO的优化原理, 并举了BattleField3的STF为例.</p>
<h2>引用</h2>
<p>Nehab, Sander, Lawrence, Tatarchuk, Isidoro. Accelerating Real-Time Shading with Reverse Reprojection Caching. In ACM SIGGRAPH/Eurographics Symposium on Graphics Hardware 2007.</p>
<p>Mattausch, Oliver, Daniel Scherzer, and Michael Wimmer. &quot;High‐Quality Screen‐Space Ambient Occlusion using Temporal Coherence.&quot; Computer Graphics Forum. Vol. 29. No. 8. Blackwell Publishing Ltd, 2010.</p>
<p>BAVOIL L., SAINZ M.: Multi-layer dual-resolution screen-space ambient occlusion. In SIGGRAPH ’09: SIGGRAPH 2009: Talks (New York, NY, USA, 2009), ACM, pp. 1–1. 3, 7</p>
<p>Screen space ambient occlusion. (2016, December 29). In Wikipedia, The Free Encyclopedia. Retrieved 19:24, January 28, 2017, from Screen space ambient occlusion</p>
<p>Bavoil, L., &amp; Sainz, M. (2008). Screen space ambient occlusion. NVIDIA developer information: http://developers. nvidia. com, 6.</p>
]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;/assets/js/DPlayer.min.js&quot;&gt; &lt;/script&gt;&lt;p&gt;本文同时也发表在我的&lt;a href=&quot;https://zhuanlan.zhihu.com/p/25038820?refer=MeowShader&quot; target=&quot;_bla
    
    </summary>
    
    
      <category term="Unity" scheme="http://www.weymire.com/tags/Unity/"/>
    
      <category term="Computer Graphics" scheme="http://www.weymire.com/tags/Computer-Graphics/"/>
    
      <category term="Shader" scheme="http://www.weymire.com/tags/Shader/"/>
    
      <category term="Siggraph" scheme="http://www.weymire.com/tags/Siggraph/"/>
    
  </entry>
  
  <entry>
    <title>Vertical Fog及在Fragment Shader中重建世界坐标</title>
    <link href="http://www.weymire.com/2016/12/04/essay2/"/>
    <id>http://www.weymire.com/2016/12/04/essay2/</id>
    <published>2016-12-04T11:15:00.000Z</published>
    <updated>2017-02-12T12:06:08.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><p>本文同时也发表在我的<a href="https://zhuanlan.zhihu.com/p/24160707?refer=MeowShader" target="_blank" rel="external">知乎专栏</a>上😄.</p>
<blockquote>
<p>作者：音速键盘猫</p>
<p>链接：https://zhuanlan.zhihu.com/p/24160707?refer=MeowShader</p>
<p>来源：知乎</p>
<p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
</blockquote>
<h2>Vertical Fog (垂直雾) 是什么</h2>
<p>想必大家都知道雾特效, 一般来讲, 距离摄像机越远的点, 其受到雾特效的影响会越为严重. 这是最为常见的雾特效.</p>
<p><img src="/image/essay2/8.jpg" alt=""></p>
<p>但是还有一种雾, 在某一点的浓度与其和观察点间的距离关系似乎并不大, 而与其<strong>世界位置坐标</strong>有非常紧密的联系.</p>
<p><img src="/image/essay2/1.jpg" alt="">
<img src="/image/essay2/2.jpg" alt="">
<img src="/image/essay2/3.jpg" alt=""></p>
<p>我不太懂物理, 姑且理解为雾霾因自身受到的重力而产生了沉积作用, 使得距离地面较近的区域雾浓度特别高, 但是一旦高过某个阈值, 浓度则开始急剧下降.</p>
<p>以上属于我的不懂装懂辣鸡解释. 如果某位大侠能够科学地解释一下, 感激不尽.</p>
<p>以上三张照片全部来自谷歌图片, 摄于<em>Dubai</em>.</p>
<p>知道了Vertical Fog是什么东西之后, 我们就需要知道这个特效有什么用.</p>
<p>在很多Top-Down类型的游戏(比如LOL, Dota, Space Marshall)中想要加入雾特效的话, 使用传统的基于距离和深度的雾特效会导致效果失真. 这是因为Top-Down的视角是比较广的, 简单粗暴地糊上一层雾会导致许多距离摄像机较远, 但又很重要的部分被渲染为白茫茫的雾. 与此相反, 我们只希望在放置GameObject的那一层产生比较集中的雾, 因此可以考虑基于每一个点的世界坐标决定其雾的浓度.</p>
<p>本文将要实现基于Image Effect的Vertical Fog效果. 当然了思路是相通的, 如果想要局部地添加雾特效, 也可以将类似的着色器特效应用于模型上, 然后注意调整模型的Blend与ZTest就好了.</p>
<h2>猜一猜这个特效要怎么实现</h2>
<p>恩, 假设我们什么都不知道, 只有一个处理前的图和处理后的效果图:</p>
<p><img src="/image/essay2/4.png" alt=""></p>
<p>原图</p>
<p><img src="/image/essay2/5.png" alt=""></p>
<p>处理后</p>
<p>通过上面的两张图, 我们能得到如下的结论:</p>
<ul>
<li>1	某点的雾的浓度, 和该点的世界Y坐标有关系.</li>
<li>2	某点的雾的浓度, 和摄像机的位置, FOV, 角度等都没有任何关系.</li>
<li>3	雾的浓度符合某种数学公式, 使其沉积在了比较低的区域.</li>
<li>4	这个特效适合俯视被观察区域的情况. 如果身在雾中的话, 恐怕什么都看不清楚.</li>
</ul>
<p>很明显, 只要能知道某一点的世界Y坐标, 那什么问题都解决了.</p>
<h2>一个Naive的思路 (直白而低效)</h2>
<p>在片元着色器中逆推出每一个片元的View Space坐标, 然后乘以_InverseView矩阵将之转化回World Space.</p>
<p>首先我们将片元屏幕坐标重新映射到[-1, 1]的区间以回归到NDC Space, 随后将NDC Space转化为View Space, 再通过从C#脚本传入的摄像机的世界-摄像机变换矩阵的逆求出其世界坐标.😏</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line"><span class="keyword">float</span> depth = LinearEyeDepth ( SAMPLE_DEPTH_TEXTURE( _CameraDepthTexture, i.uv ) ); </div><div class="line">float2 p11_22 = float2 ( unity_CameraProjection._11, unity_CameraProjection._22 ); </div><div class="line">float3 vpos = float3( ( i.uv * <span class="number">2</span> - <span class="number">1</span>) / p11_22, <span class="number">-1</span> ) * depth; </div><div class="line">float4 wpos = mul( _InverseView, float4( vpos, <span class="number">1</span> ) ); </div><div class="line"><span class="keyword">return</span> wpos.y / <span class="number">10</span>;<span class="comment">//下文中会说到为什么写得这么奇怪.</span></div></pre></td></tr></table></figure></p>
<p>这段程序中vpos的计算过程为了让代码看起来简单点而做了一点变化, 更加直观的方式是这样的:</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line">float2 ndc = i.uv * <span class="number">2</span> - <span class="number">1</span>;</div><div class="line">float3 vpos;</div><div class="line">vpos.x = ndc / p11_22.x * depth;</div><div class="line">vpos.y = ndc / p11_22.y * depth;</div><div class="line">vpos.z = -depth;</div></pre></td></tr></table></figure></p>
<p>unity_CameraProjection中存储的是投影矩阵. (这个是OpenGL的版本)其形式如下:</p>
<p><span>$$\begin{pmatrix} \frac {2n} {r - l} &amp; 0 &amp; \frac {r + l} {r - l} &amp; 0 \\
&Tab;0 &amp; \frac {2n} {t - b} &amp; \frac {t + b} {t - b} &amp; 0 \\
&Tab;0 &amp; 0 &amp; \frac {-(f + n)} {f - n} &amp; \frac {-2fn} {f - n} \\ 
&Tab;0 &amp; 0 &amp; -1 &amp; 0 \\
\end{pmatrix}$$</span><!-- Has MathJax --></p>
<p>点元着色器的坐标变化处理过程如下:</p>
<p><img src="/image/essay2/6.png" alt=""></p>
<p>(这个图是DirectX的版本, 换到OpenGL的话, 倒数第二个框框中y的处理应该和x一样)</p>
<p>我们现在相当于在倒数第二个框框内, 唯一不同的是我们采用的是uv坐标, 范围是[0, 1]而并非[0, width]与[0, height]. 因此第二段程序所做的, 就是利用uv坐标来求出NDC Space坐标. 注意, 到此为止完全和Z没有任何关系. 所以我们只需要让x分量除以$\frac {2n} {r - l}$, 让y分量除以$\frac {2n} {t - b}$即可.</p>
<p>转化回NDC Space后, 由于我们本质上已经做过了标准化和剪裁, 因此倒数第四个与第五个框框跳过, 我们的逆推过程进入到了蓝色的大框框中. 而根据坐标变换规则, 我们有如下等式:</p>
<p>$$
x^{'} = \frac {nP_{x}} {-P_{z}}
$$
$$
y^{'} = \frac {nP_{y}} {-P_{z}}
$$
$$
z^{'} = n
$$</p>
<p>OpenGL中View Space的Z轴正方向背离View Frustum. 而通过CameraDepthTexture我们得到的值均为正, 因此需要特殊变换一下.</p>
<p>OK, 到此为止我们已经成功将Screen Space丢到了View Space中, 我们只需要在C#脚本中插入如下代码, 就可以将世界-摄像机变换矩阵的逆传入:</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">material.SetMatrix ( &quot;_InverseView&quot;, GetComponent&lt;Camera&gt; ().cameraToWorldMatrix );</div></pre></td></tr></table></figure></p>
<p>然后, 乘以这个矩阵即可:</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line">float4 wpos = mul ( _InverseView, vpos );</div></pre></td></tr></table></figure></p>
<p>现在说明下为什么我们最后要查看的是<strong>wpos.y / 10</strong>. 其实这个10是我顺手敲上去的😆, 人眼对于暗色的分辨能力高于对明亮颜色的分辨能力, 因此这个过程非常类似于Gamma Correction. 但是为什么我这没有用乘方的形式进行校正呢? 这是因为严格意义上来讲我们输出的是&quot;坐标&quot;, 而游戏场景中的坐标可能会比较大, 比如厨房的柜子顶端其y轴坐标就达到了<strong>2.5m</strong>. 因此不如简单粗暴地除以10, 这样也很容易查看我们最后的结果是否正常😝.</p>
<p><img src="/image/essay2/7.png" alt=""></p>
<p>如图所示, 越高的地方越明亮.</p>
<h2>但是, 我们不希望Naive</h2>
<h3>分析</h3>
<p>为什么上面介绍的方法不好? 我们都知道矩阵乘法是一个颇为耗资源的一个操作, 哪怕搬到GPU上也一样. 而上面的做法是在片元着色器中做坐标转换, <strong>屏幕分辨率是多少就做了多少次矩阵乘法</strong> ... 😅</p>
<h3>方法</h3>
<p>不知道读者是不是和我一样有一种感觉: 一个特定的ViewPort Position和一个特定的深度值, 是能够唯一确定一个世界坐标的. 我不会画图 ... 诸位脑补一下哈, 透视投影的过程中, 处在同一条从摄像机射出的射线上的点, 最终会被绘制到同一个位置上. (这也就是深度测试的意义之一 --- 只让最近的那个点被绘制出来). 但是如果我们又同时知道了射线上的某个点到摄像机的距离, 那么这个点就是唯一确定的😠.</p>
<p>那么最后我们要得到的世界坐标就是$ray * depth + _WorldSpaceCameraPos$.</p>
<p>恩, 如果能快速得到这条射线就好了. 其实得到这条射线的方法简单的令人发指😚.</p>
<ul>
<li>我们可以在C#脚本中计算出摄像机到其View Frustum的远剪裁面的四个角的世界坐标射线.</li>
<li>对于全屏幕的后期特效, 其实就是一个全屏幕的Quad, 四个顶点. 一个顶点对应上述的一个角.</li>
<li>点元着色器输出至片元着色器的过程中自带插值 ... 我们什么都不用做, 就这么华丽丽地得到了想要的射线.</li>
</ul>
<p>求View Frustum四个角的世界坐标的C#程序:</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Matrix4x4 frustumCorners = Matrix4x4.identity; </div><div class="line">float fovWHalf = camFov * 0.5f; </div><div class="line">Vector3 toRight = m_camTrans.right * camNear * Mathf.Tan (fovWHalf * Mathf.Deg2Rad) * camAspect; </div><div class="line">Vector3 toTop = m_camTrans.up * camNear * Mathf.Tan (fovWHalf * Mathf.Deg2Rad); </div><div class="line">Vector3 topLeft = (m_camTrans.forward * camNear - toRight + toTop); </div><div class="line">float camScale = topLeft.magnitude * camFar/camNear; </div><div class="line">topLeft.Normalize(); </div><div class="line">topLeft *= camScale; </div><div class="line">Vector3 topRight = (m_camTrans.forward * camNear + toRight + toTop); </div><div class="line">topRight.Normalize(); </div><div class="line">topRight *= camScale; Vector3 bottomRight = (m_camTrans.forward * camNear + toRight - toTop); </div><div class="line">bottomRight.Normalize(); </div><div class="line">bottomRight *= camScale; </div><div class="line">Vector3 bottomLeft = (m_camTrans.forward * camNear - toRight - toTop);</div><div class="line">bottomLeft.Normalize(); bottomLeft *= camScale; </div><div class="line">frustumCorners.SetRow (0, topLeft); </div><div class="line">frustumCorners.SetRow (1, topRight); </div><div class="line">frustumCorners.SetRow (2, bottomRight); </div><div class="line">frustumCorners.SetRow (3, bottomLeft); </div><div class="line">material.SetMatrix (&quot;_FrustumCornersWS&quot;, frustumCorners);</div></pre></td></tr></table></figure></p>
<p>现在我们的问题是如何让这个矩阵代表的四个角与Screen Quad的四个角一一对应.</p>
<p>通过观察, 我们得到了如下关系:</p>
<p>uv.x = 0, uv.y = 0 ------ index = 3;</p>
<p>uv.x = 1, uv.y = 0 ------ index = 2;</p>
<p>uv.x = 1, uv.x = 1 ------ index = 1;</p>
<p>uv.x = 0, uv.y = 1 ------ index = 0;</p>
<p>我们需要知道一个函数F(x, y) = index, 使其能符合上述关系. 否则我们在点元着色器中就要使用if来判断x与y的关系, 从而和z一一对应. 摆脱了矩阵乘法, 然后引入了一坨if ... 这波真亏.
容易推知如下关系: $F(x, y) = abs (3 - x - 3 * y)$.</p>
<h3>重获新生的点元着色器</h3>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line"><span class="function">v2f <span class="title">vert</span> <span class="params">(appdata_img v)</span> </span></div><div class="line">&#123; </div><div class="line">	v2f o; </div><div class="line">	o.vertex = mul(UNITY_MATRIX_MVP, v.vertex); </div><div class="line">	o.uv = v.texcoord.xy; </div><div class="line">	<span class="keyword">int</span> xx = (<span class="keyword">int</span>)v.vertex.x; <span class="keyword">int</span> yy = (<span class="keyword">int</span>)v.vertex.y; </div><div class="line">	<span class="keyword">int</span> z = <span class="built_in">abs</span> (<span class="number">3</span> - xx - <span class="number">3</span> * yy); </div><div class="line">	o.interpolatedRay = _FrustumCornersWS[ z ]; </div><div class="line">	o.interpolatedRay.w = ( v.vertex.z ); <span class="keyword">return</span> o; </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2>计算雾的浓度</h2>
<p>首先我们获取每一个片元的世界坐标. 由于射线射向的是远剪裁面, 因此这里将DepthBuffer Linearize的时候不要转化成EyeSpace ... 应该是01Space.</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line"><span class="keyword">float</span> depth = Linear01Depth ( SAMPLE_DEPTH_TEXTURE ( _CameraDepthTexture, UnityStereoScreenSpaceUVAdjust ( i.uv, _CameraDepthTexture_ST ) ) ); </div><div class="line">float3 worldPos = ( depth * i.interpolatedRay ).xyz + _WorldSpaceCameraPos;</div></pre></td></tr></table></figure></p>
<p>然后应用一个随着高度指数衰减的密度函数就可以了, 在这里我随便写了一个, 仅供参考啦😸:</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line"><span class="keyword">return</span> lerp (tex2D (_MainTex, i.uv), _FogColor, saturate(<span class="built_in">exp</span>(-worldPos.y - _Start) * _Density));</div></pre></td></tr></table></figure></p>
<h3>最终效果</h3>
<p><img src="/image/essay2/5.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;/assets/js/DPlayer.min.js&quot;&gt; &lt;/script&gt;&lt;p&gt;本文同时也发表在我的&lt;a href=&quot;https://zhuanlan.zhihu.com/p/24160707?refer=MeowShader&quot; target=&quot;_bla
    
    </summary>
    
    
      <category term="Unity" scheme="http://www.weymire.com/tags/Unity/"/>
    
      <category term="Computer Graphics" scheme="http://www.weymire.com/tags/Computer-Graphics/"/>
    
      <category term="Shader" scheme="http://www.weymire.com/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>相交高亮(扫描效果)</title>
    <link href="http://www.weymire.com/2016/11/27/essay3/"/>
    <id>http://www.weymire.com/2016/11/27/essay3/</id>
    <published>2016-11-27T11:15:00.000Z</published>
    <updated>2017-02-12T10:11:39.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><p>本文同时也发表在我的<a href="https://zhuanlan.zhihu.com/p/23971284?refer=MeowShader" target="_blank" rel="external">知乎专栏</a>上😄. 不过当时写作的风格实在不适合技术文, 因此在个人博客中的这篇文章经过了一些修改.</p>
<blockquote>
<p>作者：音速键盘猫</p>
<p>链接：https://zhuanlan.zhihu.com/p/23971284?refer=MeowShader</p>
<p>来源：知乎</p>
<p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
</blockquote>
<h2>相交高亮(Intersection Highlight)是什么</h2>
<p>相交高亮, 是一种附加在Mesh上的着色器特效, 其功能是将所有其他穿过该Mesh表面的截面轮廓绘制出来, 产生一种类似于扫描一样的效果. 多用于<strong>科幻类游戏</strong>中.</p>
<p><img src="/image/essay3/2.jpg" alt=""></p>
<p>在这里我要检讨下 ... 这种相交高亮的特效出现的频次真心不低, 我能确切想起来的实际应用游戏便有<em>杀戮地带</em>系列, <em>质量效应</em>系列, <em>泰坦陨落</em>系列和<em>死亡空间</em>系列. 不过在youtube上找了好几圈也没有找到一个包含了这个效果的视频 ... 所以说只能贴上这张图啦.</p>
<h2>相交高亮着色器工作原理</h2>
<p>获取当前摄像机渲染的场景的<strong>DepthBuffer</strong>, 在渲染当前模型的时候判断每一个经过坐标变换的片元的世界坐标Z是否和DepthBuffer的对应点深度足够接近. 如果足够接近, 则将其渲染成另一种颜色.</p>
<h2>猜一猜实现的方法</h2>
<p><img src="/image/essay3/3.png" alt=""></p>
<p>我们要实现的就是那个正方体的材质. 从图中我们清楚的看到, 水壶, 电脑和键盘在正方体外面的部分是非常正常的, 在正方体内部的部分蒙上了一层黄色. <strong>但是和正方体的相交截面的外轮廓被绘制成了蓝颜色</strong>.</p>
<p>到此, 我们能够推断出来的事实有:</p>
<ul>
<li>
<p>1	Blend Mode为: Blend SrcAlpha OneMinusSrcAlpha 原因很简单, 因为我们能够通过正方体看到其后面的物体, 这说明正方体本身的颜色和原本的ColorBuffer的Alpha值被&quot;平分秋色&quot;后进行了混合. 依然看不懂的童鞋请参见<a href="https://docs.unity3d.com/Manual/SL-Blend.html" target="_blank" rel="external">Unity官方文档</a>.</p>
</li>
<li>
<p>2	RenderQueue为Transparent 很明显, 我们当然是希望这个正方体在Geometry后渲染出来, 这样才能透过它看到优先渲染的Opaque Materials. 关于Render Order的详细描述可以看<a href="https://docs.unity3d.com/462/Documentation/Manual/SL-SubshaderTags.html" target="_blank" rel="external">Unity官方文档</a>.</p>
</li>
<li>
<p>3	正因为我们的正方体是被后渲染出来的, 所以我们可以通过当前的ColorBuffer或者是DepthBuffer等资源来以某种方式处理相交截面.</p>
</li>
<li>
<p>4	但是不管截面到底是怎么被处理出来的, 我们必须得知道屏幕上某点的世界坐标相对于正方体某个片元的世界坐标的相对关系.</p>
</li>
</ul>
<p>很明显, 我们要做的就是优雅地解决第四个问题.</p>
<h2>如何优雅地求坐标</h2>
<p>可以通过DepthBuffer, 摄像机Near Clip Plane, Far Clip Plane, Field of View来计算出屏幕上每一个点的世界坐标, 但是传统的在片元着色器中计算世界坐标的方式是处理后依次乘以世界-视图矩阵的逆, 效率堪忧. 就算利用点元着色器预先计算视椎体射线, 效率有了些许提升, 也远远达不到&quot;优雅&quot;的水准.
(PS: 我会在后面的文章中详细介绍Global Fog后期处理特效, 其中会对在片元着色器中通过DepthBuffer计算世界坐标的方法展开讨论. )</p>
<p>说了这么多, 我们发现直接求世界坐标这种套路最直接, 最好理解, 但似乎并不太可取. 那么我们就要思考一个问题: 我们真的必须得知道具体的世界坐标嘛?</p>
<p>通过观察上面的那张图, 我们发现为了确定如何渲染并混合颜色, 我们只需要知道相对于摄像机来讲, 正方体的片元和原本场景中对应位置的像素谁离得更远就行了. <strong>也就是说, 我们只需要知道两个三维向量的长度, 也就是两个实数, 而并不需要知道这两个三维向量的xyz都分别是什么</strong>.</p>
<p>所以说, 求世界坐标的话有点儿杀鸡用牛刀了.</p>
<h2>如何获取一个片元所在屏幕坐标的DepthBuffer</h2>
<p><img src="/image/essay3/4.png" alt=""></p>
<p>我们是如何知道一个片元的投影坐标的呢?</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line">o.pos = mul ( UNITY_MATRIX_MVP, v.vertex );</div></pre></td></tr></table></figure></p>
<p>那么我们要怎么通过世界坐标来将其xy分量映射到[0, 1]区间呢? 毕竟只有这样我们才能采样DepthBuffer啊! 在UnityCG.cginc中, 有这么个函数:</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> float4 <span class="title">ComputeNonStereoScreenPos</span><span class="params">(float4 pos)</span> </span></div><div class="line">&#123; </div><div class="line">	float4 o = pos * <span class="number">0.5f</span>; </div><div class="line">	<span class="meta">#<span class="meta-keyword">if</span> defined(UNITY_HALF_TEXEL_OFFSET) </span></div><div class="line">	o.xy = float2(o.x, o.y*_ProjectionParams.x) + o.w * _ScreenParams.zw; </div><div class="line">	<span class="meta">#<span class="meta-keyword">else</span> </span></div><div class="line">	o.xy = float2(o.x, o.y*_ProjectionParams.x) + o.w; </div><div class="line">	<span class="meta">#<span class="meta-keyword">endif</span> o.zw = pos.zw; </span></div><div class="line">	<span class="keyword">return</span> o; </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>多数情况下上述代码也可以直接写成以下形式:</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> float4 <span class="title">ComputeScreenPos</span> <span class="params">(float4 pos)</span> </span></div><div class="line">&#123; </div><div class="line">	float4 o = pos * <span class="number">0.5</span>; </div><div class="line">	o.xy = float2(o.x, o.y*_ProjectionParams.x) + o.w; </div><div class="line">	o.zw = pos.zw; </div><div class="line">	<span class="keyword">return</span> o; </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>看起来有点复杂 ... 先乘个0.5然后再加上w分量的二分之一 ... ?</p>
<h2>&quot;乘以0.5然后加上w分量的二分之一&quot;到底是什么</h2>
<p>首先上个图</p>
<p><img src="/image/essay3/5.png" alt=""></p>
<p>在Unity中, mul ( UNITY_MATRIX_MVP, v.vertex )和UnityObjectToClipPos(float4 ( v.vertex.xyz, 1.0 ) )干的差不多都是一回事儿, 就是将模型坐标转换到摄像机的Homogeneous Clip Space. 详情参见<a href="https://docs.unity3d.com/Manual/SL-BuiltinFunctions.html" target="_blank" rel="external">官方文档</a>(其实我个人更加推荐用UnityObjectToClipPos来代替传统的乘以MVP矩阵. 尤其是涉及到用GPU Generate Object的时候, 直接乘以MVP矩阵往往得到的是错误的结果).</p>
<p>但是, 一般渲染管线不会立刻将Clip后的坐标标准化(也就是除以w分量), 而是在点元着色函数结束以后将其标准化. 这个地方有点坑.</p>
<blockquote>
<p>Once all the vertices are transformed to clip space a final operation called perspective division is performed where we divide the x, y and z components of the position vectors by the vector's homogeneous w component; perspective division is what transforms the 4D clip space coordinates to 3D normalized device coordinates. <strong>This step is performed automatically at the end of each vertex shader run</strong>.</p>
</blockquote>
<p>所以, 可以认为我们现在得到的是已经经过Clipping, 但是还没有标准化的投影坐标. 我们的目的是要将这个坐标转化为xy在[0, 1]之间, 而z反映深度的屏幕坐标.</p>
<p>既然是[0, 1]之间, 那么我们自然就不用向上图一样乘以ViewPort宽高了. 同时要注意ViewPort坐标原点的问题: Unity中是左下角, 而上图采用的是左上角. 所以具体到我们的情况下y和x的处理方式应该是相同的.</p>
<p>为了将[-1, 1]映射到[0, 1]上, 将原坐标加1然后除2是显而易见的. 但是要注意我们的x和y都比人家多乘着一个w分量. 因为运算到这个时候我们依然在点元着色器函数中, <strong>因此最终的标准化过程还没有执行</strong>.</p>
<p>所以, 我们就得到了下面这段代码(其实也是上面那段)</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line">float4 o = pos * <span class="number">0.5</span>;</div><div class="line">o.xy = float2(o.x, o.y*_ProjectionParams.x) + o.w;</div></pre></td></tr></table></figure></p>
<h2>点元着色器</h2>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line"><span class="function">v2f <span class="title">vert</span> <span class="params">( appdata_base v )</span> </span></div><div class="line">&#123; </div><div class="line">	v2f o; </div><div class="line">	o.pos = UnityObjectToClipPos ( v.vertex ); </div><div class="line">	o.projPos = ComputeScreenPos ( o.pos ); </div><div class="line">	<span class="keyword">return</span> o; </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2>非常简单的片元着色器函数</h2>
<p>在片元着色器中, 我们只需要提取出对应屏幕位置的深度信息, 然后和点元着色器的输出深度信息作比较, 根据相差结果进行插值即可.</p>
<p><figure class="highlight"><table><tr><td class="code"><pre><div class="line">fixed4 frag ( v2f i ) : SV_TARGET </div><div class="line">&#123; </div><div class="line">	float4 finalColor = _MainColor; </div><div class="line">	float sceneZ = LinearEyeDepth (tex2Dproj(_CameraDepthTexture, UNITY_PROJ_COORD(i.projPos))); </div><div class="line">	float partZ = i.projPos.z; </div><div class="line">	float diff = min ( (abs(sceneZ - partZ)) / _Threshold, 1); </div><div class="line">	finalColor = lerp(_HighlightColor, _MainColor, diff); </div><div class="line">	return finalColor; </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2>最终效果</h2>
<p><img src="/image/essay3/6.png" alt=""></p>
<h2>后记</h2>
<p>其实这个特效的原理真心一点也不复杂, 只是用到了DepthTexture来获取屏幕中每个像素的深度信息来进行比对以决定模型最终的颜色. 但是UnityCG.cginc里面的ComputeScreenPos函数那个奇怪的外观引发了我极大的好奇心.</p>
]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;/assets/js/DPlayer.min.js&quot;&gt; &lt;/script&gt;&lt;p&gt;本文同时也发表在我的&lt;a href=&quot;https://zhuanlan.zhihu.com/p/23971284?refer=MeowShader&quot; target=&quot;_bla
    
    </summary>
    
    
      <category term="Unity" scheme="http://www.weymire.com/tags/Unity/"/>
    
      <category term="Computer Graphics" scheme="http://www.weymire.com/tags/Computer-Graphics/"/>
    
      <category term="Shader" scheme="http://www.weymire.com/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>镜头炫光与光晕</title>
    <link href="http://www.weymire.com/2016/11/26/essay4/"/>
    <id>http://www.weymire.com/2016/11/26/essay4/</id>
    <published>2016-11-26T11:15:00.000Z</published>
    <updated>2017-02-12T10:49:51.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><p>本文同时也发表在我的<a href="https://zhuanlan.zhihu.com/p/23950559?refer=MeowShader" target="_blank" rel="external">知乎专栏</a>上😄. 不过当时写作的风格实在不适合技术文, 因此在个人博客中的这篇文章经过了一些修改.</p>
<blockquote>
<p>作者：音速键盘猫</p>
<p>链接：https://zhuanlan.zhihu.com/p/23971284?refer=MeowShader</p>
<p>来源：知乎</p>
<p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
</blockquote>
<h2>镜头光晕和炫光是什么?</h2>
<p>我们都知道, 光在传播的过程中经过两种折射率不同的介质交界面时, 会发生折射和反射现象. 当折射和反射达成某种角度, 尤其是光比比较大时, 反射光和折射光可以交汇形成炫光. 在多数情况下, 这并不是一个好的现象, 因为这导致被观察物体的表现形式失真. 现在大多数镜头的镜片上都会有各种各样的镀膜, 为的就是减少炫光的出现.</p>
<p><img src="/image/essay4/1.jpg" alt=""></p>
<p>如上图, 这就是一个标准的镜头炫光的结果. 这种结果当然不会是我们想要的.</p>
<p><img src="/image/essay4/2.jpg" alt=""></p>
<p>当然了, 如果使用得当, 镜头光晕也可以像这样 ...</p>
<p>那么, 我们为什么要做一个镜头炫光的特效呢?</p>
<p>第一, 虽然镜头炫光是人造效果, 但是它能够增加一张图片的<strong>动态范围</strong>, 使其更加直观和清晰地阐述明亮度.</p>
<p>第二, 镜头炫光给人一种意境化的感觉, 使用得当可以让人更加<strong>身临其境</strong>.</p>
<p>第三, 镜头炫光看起来真的很<strong>酷</strong> ...</p>
<h2>实现的基本思路</h2>
<p>根据一个RenderTexture(一般简称为RT, 也可以称作Render Target)的颜色信息, 通过后期图像特效(Image Effect)计算产生一张带有其镜头光晕的RT, 然后将后产生的RT合并到原本的RT中通过屏幕输出.</p>
<p>文章中介绍的做法<strong>不是基于物理</strong>的, 因此只能近似地模拟炫光效果. 但是性能开销非常廉价, 可以应用到移动端上. 效果也还可以.</p>
<h2>特效的基本流程</h2>
<p>第一步, 根据一个阈值提取图像中的所有明亮度高的像素, 并将结果适当降采样(DownSample)以提升时间效率. 关于降采样, Unity的Standard Assets中的后期特效中有降采样的源代码.</p>
<p>第二步, 基于第一步得到的RT, 计算出对应的鬼影位置.</p>
<p>第三步, 对第二步得到的RT进行高斯模糊, 而后进行星射线采样处理, 得到真正的镜头炫光.</p>
<p>第四步, 将第三步的RT和原有ColorBuffer进行混合, 投射到屏幕上.</p>
<h2>Step 1: 降采样和像素提取</h2>
<p>降采样是为了以牺牲图像质量为代价来降低后续操作的性能开销. 在后面的操作过程中我们涉及到了高斯模糊, 而顾名思义反正早晚要模糊, 那干脆输入一个本来就有点糊(注意, 我说是&quot;有点&quot;), 但是尺寸小很多的RT该多么划算. 因此可以先用将原图采样到一个尺寸更小的图片中的方式来压缩原图. 思路是使用一个长宽等比例缩小k倍的RT, 在着色器实现过程中每隔原图的k个像素点采样一次, 放到新的降采样RT中. 具体到程序实现的话可以如此理解: 将降采样RT&quot;强行&quot;拉伸到原图的尺寸, 那么相邻两个片元的UV坐标差距也就增加到了原来的K倍. 因此我们可以利用降采样RT片元的UV坐标来采样原图. 为了保证降采样后的效果, 我们将原图对应部分的上下左右四个像素点求平均值进行输出. (在Unity官方Shader中是采用对应部分像素点和其左边, 右边, 左下角共四个像素点求平均值, 在最终的视觉效果上感觉也没什么差异, 但是取上下左右似乎更符合&quot;对称&quot;的强迫症思想).</p>
<p>在降采样后, 我们需要从中提取一些明亮度较高的像素. 因为我们只希望将图片中特别明亮的部分进行镜头光晕化的处理. 具体做法就很简单了, 降采样后的颜色像素RGB超过阈值的部分提取出来.</p>
<p><figure class="highlight"><table><tr><td class="code"><pre><div class="line">struct v2f_DownSample </div><div class="line">&#123; </div><div class="line">	float4 pos : SV_POSITION; </div><div class="line">	half2 uv20 : TEXCOORD0; </div><div class="line">	half2 uv21 : TEXCOORD1; </div><div class="line">	half2 uv22 : TEXCOORD2; </div><div class="line">	half2 uv23 : TEXCOORD3; </div><div class="line">&#125;; </div><div class="line"></div><div class="line">v2f_DownSample vert_DownSample ( appdata_img v ) </div><div class="line">&#123; </div><div class="line">	v2f_DownSample o; </div><div class="line">	o.pos = mul ( UNITY_MATRIX_MVP, v.vertex ); </div><div class="line">	o.uv20 = UnityStereoScreenSpaceUVAdjust ( v.texcoord + _MainTex_TexelSize * half2 ( 0.5h, 0.5h ), _MainTex_ST ); </div><div class="line">	o.uv21 = UnityStereoScreenSpaceUVAdjust ( v.texcoord + _MainTex_TexelSize * half2 ( -0.5h, 0.5h ), _MainTex_ST ); </div><div class="line">	o.uv22 = UnityStereoScreenSpaceUVAdjust ( v.texcoord + _MainTex_TexelSize * half2 ( -0.5h, -0.5h ), _MainTex_ST ); </div><div class="line">	o.uv23 = UnityStereoScreenSpaceUVAdjust ( v.texcoord + _MainTex_TexelSize * half2 ( 0.5h, -0.5h ), _MainTex_ST ); </div><div class="line">	return o; </div><div class="line">&#125; </div><div class="line"></div><div class="line">fixed4 frag_DownSample ( v2f_DownSample i ) : COLOR </div><div class="line">&#123; </div><div class="line">	fixed4 color = tex2D ( _MainTex, i.uv20 ) + tex2D ( _MainTex, i.uv21 ) + tex2D ( _MainTex, i.uv22 ) + tex2D ( _MainTex, i.uv23 ); </div><div class="line">	return max ( 0.0, color / 4 - _Threshold ) * _Intensity; </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>对应的C#脚本:</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">int rtWidth = source.width &gt;&gt; downSampleNum; </div><div class="line">int rtHright = source.height &gt;&gt; downSampleNum; </div><div class="line">RenderTexture downSampleBuffer = RenderTexture.GetTemporary(rtWidth, rtHright, 0, source.format); downSampleBuffer.filterMode = FilterMode.Bilinear;</div></pre></td></tr></table></figure></p>
<p>成果如下:</p>
<p><img src="/image/essay4/3.png" alt=""></p>
<p>(原图)</p>
<p><img src="/image/essay4/4.png" alt=""></p>
<p>(提取高亮颜色)</p>
<p><img src="/image/essay4/5.png" alt=""></p>
<p>(降采样后)</p>
<h2>Step 2: 计算鬼影</h2>
<p>鬼影是啥? 听起来好恐怖的样子, 不会让Unity崩溃吧?</p>
<p>简单来说, 我们要实现的鬼影就是将第一步中得到的RT的明亮部分错位重复渲染几次, 达到模拟多层镜片反射的效果. 在这里我们默认对称中心就是我们的原图图像中心(当然因此也就同时是降采样后的RT中心).</p>
<p>OK, 我们先来个简单的版本, 只渲染一个鬼影, 和原本的像素位置相对于图片中心呈中心对称. 实现上很简单, 重新采样一次然后和混合相加就好:</p>
<p><img src="/image/essay4/6.png" alt=""></p>
<p>这似乎有点太无聊了, 我们应该再加几个鬼影才能实现模拟多次反射的效果. 但是我们都知道, 在真正的镜头中, 这种鬼影越趋近于反射中心越小, 越靠近镜头边缘则越大, 相当于在中心对称的过程中&quot;又多了一小段距离&quot;. 因此我们应该引入一个鬼影发散率dis, 用以表示第i层鬼影相对于i-1层的位置和大小的偏离情况.</p>
<p>具体到实现上, 我们可以从当前像素点向画面中心连一个向量v, 很容易发现$v<em>dis</em>i$就是当前鬼影的偏离向量. 采样后相加, 我们就得到了这样的结果:</p>
<p><img src="/image/essay4/7.png" alt=""></p>
<p>代码如下:</p>
<p><figure class="highlight"><table><tr><td class="code"><pre><div class="line">v2f_Simple vert_Simple ( appdata_img v ) </div><div class="line">&#123; </div><div class="line">	v2f_Simple o; o.pos = mul ( UNITY_MATRIX_MVP, v.vertex ); </div><div class="line">	o.uv = v.texcoord.xy; </div><div class="line">	return o; </div><div class="line">&#125; </div><div class="line"></div><div class="line">fixed4 frag_Ghost ( v2f_Simple i ) : COLOR </div><div class="line">&#123; </div><div class="line">	half2 newUV = half2 ( 1.0h, 1.0h ) - i.uv; </div><div class="line">	half2 ghostVector = ( half2 ( 0.5h, 0.5h ) - newUV ) * _GhostDispersal; </div><div class="line">	fixed4 finalColor = fixed4 ( 0, 0, 0, 0 ); </div><div class="line">	for (int ii = 0;ii &lt; _GhostNum;ii++) </div><div class="line">	&#123; </div><div class="line">		half2 offset = frac ( newUV + ghostVector * float ( ii ) ); </div><div class="line">		float weight = length ( half2 ( 0.5h, 0.5h ) - offset ) / length ( half2 ( 0.5h, 0.5h ) ); </div><div class="line">		weight = pow ( 1.0 - weight, 1.0 ); finalColor += tex2D ( _MainTex, offset ) * weight;</div><div class="line">	&#125; </div><div class="line">	return finalColor * tex2D ( _Gradient, length ( half2 ( 0.5h, 0.5h ) - newUV ) / length ( half2 ( 0.5h, 0.5h ) ) );</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2>Step 3: 模糊</h2>
<p>鬼影计算完了, 我们把它和ColorBuffer混合下看看效果吧!</p>
<p><img src="/image/essay4/8.png" alt=""></p>
<p>*能不能再不给力一点 ... ? *</p>
<p>问题出在哪?</p>
<p>太写实了! 我们要的是光斑, 光晕, 和炫光, 但是这个结果叫镜面反射!</p>
<p>那么我们接下来要做的事情就很明确了: 输入一个棱角分明的RT, 输出一个模模糊糊带着光斑的RT.</p>
<p>很好, 我猜你也想到了<strong>高斯模糊(Gaussian Blur)</strong>.</p>
<p>高斯模糊的代码就不贴了, 网上有很多. 成果如下:</p>
<p><img src="/image/essay4/9.png" alt=""></p>
<h2>Step 4: 与ColorBuffer混合</h2>
<p>我们将第三步得到的RT设置为着色器的最终鬼影纹理, 并单独在一个pass中将原图和鬼影进行混合.</p>
<p><figure class="highlight"><table><tr><td class="code"><pre><div class="line">fixed4 frag_Flare ( v2f_Simple i ) : COLOR</div><div class="line">&#123;</div><div class="line">		return tex2D ( _MainTex, i.uv ) + tex2D ( _Flare, i.uv );</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>然后我们就得到了下面的结果:</p>
<p><img src="/image/essay4/10.png" alt=""></p>
<p>虽然看起来还是有点奇怪, 不过相比于之前还是正常太多了.</p>
<h2>但是, 我们不希望它看起来奇怪</h2>
<p><img src="/image/essay4/1.jpg" alt=""></p>
<p>这个图虽然有点丑, 但是它的光晕很正常, 能够给我们一些提示:</p>
<p>首先, 这个光晕受一种&quot;星射线&quot;的影响, 给人以一种朦胧, 颤抖的感觉. 因此, 我们可以考虑为模糊后的鬼影加上一层<strong>StarBurst</strong>采样.</p>
<p><img src="/image/essay4/11.jpg" alt=""></p>
<p>再者, 通过观察我们发现, 光晕的蓝色和黄色(其实是绿色)的部分集中在中央, 而红色则扩散到了整个屏幕的各个角落. 这是因为镜头对不同波长的光有不同的折射效果. 为了实际地表现出这种效果, 我们需要做两件事情:</p>
<p>第一, 在计算鬼影的过程中, 应该允许RGB各自以不同的缩放率进行采样.</p>
<p>第二, 在鬼影计算完成后, 加入一个镜头径向采样纹理, 与鬼影相混合.</p>
<p><img src="/image/essay4/12.png" alt=""></p>
<p>(径向采样纹理, 这张纹理意味着靠近屏幕中央的鬼影更加偏向红色, 越向外越开始靠近绿色, 蓝色, 依次往复类推)</p>
<p>为了做到第一步, 我们修改下第二步中鬼影的采样即可.</p>
<p><figure class="highlight cpp"><table><tr><td class="code"><pre><div class="line">finalColor += GetDistortedColor ( offset, normalize ( ghostVector ) ) * weight;</div><div class="line"></div><div class="line"></div><div class="line"><span class="function">fixed4 <span class="title">GetDistortedColor</span> <span class="params">( half2 uv, half2 dir)</span></span></div><div class="line">    &#123;</div><div class="line">    	<span class="keyword">return</span> fixed4 (</div><div class="line">    		tex2D ( _MainTex, uv + dir * _ColorDistortion.r ).r,</div><div class="line">    		tex2D ( _MainTex, uv + dir * _ColorDistortion.g ).g,</div><div class="line">    		tex2D ( _MainTex, uv + dir * _ColorDistortion.b ).b,</div><div class="line">    		<span class="number">1</span></div><div class="line">    	);</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<p>最终成果如下:</p>
<p><img src="/image/essay4/13.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;/assets/js/DPlayer.min.js&quot;&gt; &lt;/script&gt;&lt;p&gt;本文同时也发表在我的&lt;a href=&quot;https://zhuanlan.zhihu.com/p/23950559?refer=MeowShader&quot; target=&quot;_bla
    
    </summary>
    
    
      <category term="Unity" scheme="http://www.weymire.com/tags/Unity/"/>
    
      <category term="Computer Graphics" scheme="http://www.weymire.com/tags/Computer-Graphics/"/>
    
      <category term="Shader" scheme="http://www.weymire.com/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>Magic Frame (魔拍)</title>
    <link href="http://www.weymire.com/2016/03/02/MagicFrame/"/>
    <id>http://www.weymire.com/2016/03/02/MagicFrame/</id>
    <published>2016-03-02T11:15:00.000Z</published>
    <updated>2017-03-13T18:07:15.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><h2>Abstract</h2>
<p><em>Magic Frame</em> is a 3D game developed with Unity3D as the essential engine, Maya and 3DSMAX as model makers, Photoshop and illustrator as UI designers, Visual Studio as the developing environment and C#, Javascript and Lua as the programming language. It is a multi-platform Puzzle Game available on WP, Android and iOS in which the player has to accumulate clues, figure out problems and utilize various items to find the fact behind the story. In some part of the game, the player also has to experience stealth mode to avoid being detected by robots and other devices. The game is enormously featured in its unparalleled operation method and is also distinguished by its vivid next-generation standardized models to enhance gaming experience.</p>
<h2>Trailer</h2>
<p><div id="dplayer0" class="dplayer" style="margin-bottom: 20px;"></div><script>var dplayer0 = new DPlayer({"element":document.getElementById("dplayer0"),"autoplay":false,"theme":"#FADFA3","loop":false,"video":{"url":"http://ol56no7p4.bkt.clouddn.com/MagicFrame.mp4"},"danmaku":{"api":"http://dplayer.daoapp.io","token":"tokendemo"}});</script></p>
<p>Game Introduction is Available at: <a href="http://ol56no7p4.bkt.clouddn.com/MagicFrame.pdf" target="_blank" rel="external">Magic Frame.pdf</a>. You are more than welcomed to download and have a check!</p>
<h2>Background</h2>
<p>After a two-days long sleep, our main character, Weymire woke up in a mysterious villadom. Unable to find any way out, anyone alive and any usable clues, he has to investigate more to survive and escape. He found a 3D-functional camera on his desk and it is capable of taking any photograph and take the items in the photograph into practical use. For example he can take the photo of a key, and then his camera would be capable of interacting with the corresponding lock. With this camera equipped, Weymire communicated with many memory clips and found out that the building turned out to be a secret military lab which was working on a new generation robot. The villadom is only a camouflage preventing other people from realizing its existence. Weymire was actually the top scientist in the project, yet he was also against the experiment. The robot is self-productive and could be duplicated into plenty of armies, hence it is a great threat to the human kind. At last Weymire destroyed the lab and exposed the secret to the world.</p>
<h2>Features</h2>
<p>Magic Frame is featured in the following aspects:</p>
<blockquote>
<ul>
<li>a) Creatively use gravity sensor and compass inside smart phones to control the player’s visual angle instead of the traditional solution in which the player has to use his or her right hand to rotate around the screen. Our inspiration comes from StarWalk, an astronomy introduction software available on smart phones, in which users can aim their mobile phones at a specific part of the sky and the software displays the corresponding information based on the rotation angle and the facing direction of the mobile device.</li>
<li>b) Utilize various textures, normal maps and materials to reach the standard of a next-generation mobile game. Meanwhile the use of particle effects, physical engine and dynamic lighting makes the scenes elegant and real.</li>
<li>c) Available on smart phones and supports WP, Android and iOS systems. Supports operators with x86 and ARM architectures.</li>
<li>d) The game is greatly optimized so that even lower-hardware devices could run our game perfectly. The game could run a Xiaomi 1S and iPhone 5 without any pressure.</li>
<li>e) Raise a concern about artificial-intelligence. Our game is about AI and the gaming process and ending bethinks the players about the risk of artificial- intelligence.</li>
</ul>
</blockquote>
<p>Generally speaking, Magic Frame is a multi-platform 3D puzzle game with innovative operating interface, high quality 3D models, physics systems and a concern of the development of artificial intelligence. The unique operating method also help the player to experience naturally inside the fabricated world.</p>
<h2>Core Play</h2>
<p>The visual angle is controlled by the rotation and position of the player’s mobile phone. If the player rotates his mobile phone by a certain angle upwards, the camera inside the game would also rotate by the corresponding angle upwards. Hence, the player controls his mobile device naturally during his or her whole game experience. The essential core play of Magic Frame is taking photographs. The player interact with the fabricated world by taking various photographs. For example, if the player takes a picture of an item such as a lighter, he will get the lighter and put it into his backpack. The camera that the player holds could also by implemented with other items to be more functional. For example, if the player puts the lighter into the camera and takes a photo of a candle, then the candle would be lit. As demonstrated, the camera is the media between the player and the game world inside the game, and the unique operating way is the connection between the player and the game in the real world.</p>
<p>When it comes to interactions with NPCs, the player is to take photographs based on NPC’s requirements to make them willing to talk with him. The requirements include but not limited to positions, angles and items. The player has to take a photo to a correct item at a correct position with a correct angle to be accepted. For example if an NPC requires the player to take a photo of a splashing fountain, the player has to add more water into the fountain first, and them power on the fountain and move to the right position to take the photo. Such a system increases the interactions between the player and the game.</p>
<h2>Execution</h2>
<p>As one of the most essential principles of the establishment of Mistral, we greatly take care of user experiences. In order to maximize players’ entertainment, we have innovated in the following aspects:</p>
<blockquote>
<ul>
<li>a)  Optimize the logic of scripts to reduce the expense of hardware.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>b)  Use low-poly models and integrated atlases to reduce draw-calls, triangles and vertices.</li>
<li>c) Use dynamic showing system, tutorial system, quest system, conversation system, save&amp;load system and gallery system to enhance the experience.</li>
</ul>
</blockquote>
<h2>Graphics</h2>
<p>All the models except the packages stated at the end of this document are created, constructed and ameliorated by our two 3D artists. The sense of beauty is greatly emphasized in Magic Frame, which is directly exemplified in the game.</p>
<p>Mistral also takes care of efficiency and execution. In order to reduce the expenses of hardware, we have optimized Magic Frame in many different aspects. When it comes to models, we utilized low-poly models to decrease the amount of triangles and vertices. We also integrated all the textures into 3 main atlases, which means the GPU could finish rendering all the models by calling the atlases only third times. Hence the efficiency is enormously increased.</p>
<p>When it comes to lighting and shadows, Mistral applies LightMapping to pre- bake the static objects in the scene and save the alterations caused by the lights into lightmaps, which means the GPU no longer needs to calculate these things each
frame. However, in order to minimize the negative influences caused by the optimization, the dynamic objects are quite different. When they are visible to the player, they will be rendered each frame to make sure that they look more natural and real. Also in order to make them look more elegant, they are rendered in real- time mode, which means the lighting and shadows cast upon them change each frame.</p>
<p><img src="/image/MagicFrame/1.png" alt=""></p>
<h2>Systems</h2>
<p>Our game is never a demo. It is a complete game! All the necessary systems are provided in Magic Frame. All the systems are to be elaborated on as follows:</p>
<ul>
<li>a) Tutorial System. Although the operations in Magic Frame are simple and self-explanative, the tutorial system is provided to make sure that the player can understand the operations thoroughly.
<img src="/image/MagicFrame/2.png" alt=""></li>
<li>b) Conversation System. The player could talk to the NPCs in the game.
<img src="/image/MagicFrame/3.png" alt=""></li>
<li>c) Quest System. The player could see his objectives, quests and achievements here.
<img src="/image/MagicFrame/4.png" alt=""></li>
<li>d) Reminder System. All the important reminders all here. The play could analyze the information here to find more clues.
<img src="/image/MagicFrame/5.png" alt=""></li>
<li>e) Gallery System. All the photos taken are displayed here. The player could determine which one to be saved into the system or which one to be deleted from the game.
<img src="/image/MagicFrame/6.png" alt=""></li>
<li>f) Item System. The player could check his backpack and determine which item to equip.
<img src="/image/MagicFrame/7.png" alt=""></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;/assets/js/DPlayer.min.js&quot;&gt; &lt;/script&gt;&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Magic Frame&lt;/em&gt; is a 3D game developed with Unity3D as the essen
    
    </summary>
    
    
      <category term="Unity" scheme="http://www.weymire.com/tags/Unity/"/>
    
      <category term="Project" scheme="http://www.weymire.com/tags/Project/"/>
    
      <category term="Imagine Cup" scheme="http://www.weymire.com/tags/Imagine-Cup/"/>
    
      <category term="iOS/Android" scheme="http://www.weymire.com/tags/iOS-Android/"/>
    
  </entry>
  
  <entry>
    <title>WarmWind(暖风)</title>
    <link href="http://www.weymire.com/2015/03/02/WarmWind/"/>
    <id>http://www.weymire.com/2015/03/02/WarmWind/</id>
    <published>2015-03-02T11:15:00.000Z</published>
    <updated>2017-03-13T18:07:25.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><h2>Abstract</h2>
<p><em>WarmWind</em> is a 3D game developed with <strong>Unity 3D</strong> as the essential engine, Maya and 3DSMAX as model makers, Photoshop and Illustrator as UI designers, Visual Studio as the developing environment and C# as the programming language. It is integrated by three main game genres: Puzzle game, Stealth game and Tower-defense game. Meanwhile, the game also serves as an enlightening story based on current global problems --- severe environmental conditions and the indifference among people. Hence, the game not only provides our players vivid game experiences, but also euphemistically raise their concern of social problems.</p>
<h2>Trailer</h2>
<p><div id="dplayer1" class="dplayer" style="margin-bottom: 20px;"></div><script>var dplayer1 = new DPlayer({"element":document.getElementById("dplayer1"),"autoplay":false,"theme":"#FADFA3","loop":false,"video":{"url":"http://ol56no7p4.bkt.clouddn.com/WarmWind.mp4"},"danmaku":{"api":"http://dplayer.daoapp.io","token":"tokendemo"}});</script></p>
<p>Game Introduction is Available at: <a href="http://ol56no7p4.bkt.clouddn.com/WarmWind.pdf" target="_blank" rel="external">WarmWind.pdf</a>. You are more than welcomed to download and have a check!</p>
<h2>Background</h2>
<p>Because of the factories' over-utilization of natural resources and unlimited pollution, the land on which the story starts is suffering from deforestation and harmful air conditions. Under such a circumstance, our character, Joshua, who was born in the countryside, could no longer stay with his beautiful nature because of the exacerbating environment. Both perplexed and infuriated by that, he decides to expel those factories. There is one land survived among the pollution --- The Pure Land, on which grows a Noah Seed. If the seed burgeons, the world will be back in good environment again. However, what is impending is a series of painstaking processes, since the factories are also trying to utilize the energy inside the seed. Joshua’s assignment is to do anything possible, including expelling the hazes, stealing batteries from the factory and defending the village from the invasion of robots to protect the seed until it burgeons.</p>
<h2>Conception</h2>
<p>The purpose of this game is to provide players with vivid game experiences and raise their social concerns. Meanwhile, the game differentiates itself from other games in the following aspects:</p>
<blockquote>
<ul>
<li>a)  Various playing modes and genres to attract the players all the time.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>b)  The typical use of particle effects, lights and physical engine makes the game elegant,
dreamlike and real.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>c)  Reflects problems in the real world.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>d)  Supports multi-platforms. Including but not limited to: Windows, Linux, MacOS, WP, Android and iOS.</li>
<li>e) Emphasize a sense of beauty, rather than violence and destruction.</li>
</ul>
</blockquote>
<h2>Execution</h2>
<p>As one of the most essential principles of the establishment of Mistral, we greatly take care of user experiences. In order to maximize their entertainment, we have innovated in the following aspects:</p>
<blockquote>
<ul>
<li>a)  Optimize the logic of scripts to reduce the expense of hardware.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>b)  Use low-poly models and integrated atlases to reduce draw-calls, triangles and vertices.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>c)  Natural transition from one game mode to another.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>d)  Creatively added fog blocker mode in the Puzzle mode, and weather system in the
Tower-defense mode, which makes the game totally different from other games.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>e)  Use dynamic showing system, dialogue system and quest system to give player more
guidance.</li>
</ul>
</blockquote>
<p>All of the aspects mentioned above will be discussed in detail later.</p>
<h2>Graphics</h2>
<p>All the models except the packages stated at the end of this document are created, constructed and ameliorated by our two 3D artists. The sense of beauty is greatly emphasized in WarmWind, which is directly exemplified in the game. Meanwhile, however, we also use contrast in the graphics style between the beautiful nature and the polluted area to arise the sense of duty in players’ mind.</p>
<p><img src="/image/WarmWind/image1.png" alt=""></p>
<p><img src="/image/WarmWind/image2.png" alt=""></p>
<p>Mistral also takes care of efficiency and execution. In order to reduce the expenses of hardware, we have optimized WarmWind in many different aspects. When it comes to models, we utilized low-poly models to decrease the amount of triangles and vertices. We also integrated all the textures into one main atlas, which means the GPU could finish rendering all the models by calling the atlas only one time, and thus efficiency is enormously increased.</p>
<p>When it comes to the lights and shadows, Mistral applies Lightmapping to pre-bake the static objects in the scene and save the alterations caused by the lights into lightmaps, which means the GPU no longer needs to calculate these things each frame. However, in order to minimize the negative influences caused by the optimization, the dynamic objects are quite different. When they are visible to the player, they will be rendered each frame to make sure that they look more natural and real.</p>
<h2>Engine And Programming</h2>
<p>WarmWind is developed with Unity 3D as the essential engine (as shown in Pic 3) and scripted with C#. Mistral utilized Object-Oriented Programming to create different controllers, systems, functions and game objects to constitute the game. The logic of the game is like a pyramid, which means that the game controllers are the uppermost and essential commanders on the top of the pyramid, while the mechanics such as laser-fences, cameras, enemies, mines, NPCs and items locate in the bottom. The player, dialogue system, tutorial system and quest system are the connections between the top and the bottom. Whenever a game object is triggered, it formulates a signal, transmit it through the connecting systems to the controlling center and is reasonably manipulated by making relevant alterations.</p>
<p><img src="/image/WarmWind/image3.png" alt=""></p>
<p>In order to improve the coherence and productivity of the codes, Mistral utilized Coroutine and State Machine to control animations, motions and interaction to increase efficiency. Secondary operations are manipulated during coroutines rather than main procedures so that system resources are allocated more reasonably. Furthermore, a game object could become active if and only if when corresponding signal is sent to it. The game object would cause no operations when it is deactivated, and thus trivial and unnecessary calculations are saved.</p>
<p>In order to increase the reality in the game, we also used the Physical Engine integrated in Unity 3D. Along with the dynamic lighting system and particle systems, we have created many fantastic effects.</p>
<p>We also developed the dialogue system, camera-roaming system, dynamic showing system and tutorial system to reinforce user experiences. With the help of these systems, the players would quickly know the operations in this game and start playing.</p>
<p><img src="/image/WarmWind/image4.png" alt=""></p>
<p><img src="/image/WarmWind/image5.png" alt=""></p>
<h2>Entertainment</h2>
<p>WarmWind contains various game modes to prevent the player from getting bored and keep them always active and concentrated. Experiencing with different game genres also increases entertainment. The game contains Puzzle Mode, Stealth Mode and Tower-Defense Mode.</p>
<p>In the Puzzle Mode, the player has to fight agains the hazes caused by the factory to defend the Noah Seed. The player may turn the fans and purifiers on or off, based on the electricity he has ane the coming direction of the hazes.</p>
<p><img src="/image/WarmWind/image6.png" alt=""></p>
<p>In the Stealth Mode, the player has to sneak in the dangerous factory, cautioning the robots and detection devices and steal the batteries. The player also has to deduct more information from the e-mails and letters of the workers.</p>
<p><img src="/image/WarmWind/image7.png" alt=""></p>
<p><img src="/image/WarmWind/image8.png" alt=""></p>
<p>In the Tower-Defense Mode, the player has to control the construction, upgrade and arrangement of different towers to prevent the enemies from approaching the Noah Seed. We have creatively added Weather System which could influence the speed and armor of the enemies.</p>
<p><img src="/image/WarmWind/image9.png" alt=""></p>
<p>According to the feedbacks from our 109 testing players, they regard such an arrangement reasonable and interesting. They are fully immersed into the experience of protecting the Noah Seed. Approximately 65% (71 of 109) of the players finished the whole game experience of the demo without stopping once. 80% (87 of 109) of the players regard the difficulty as very reasonable. The difficulty gradually raises and flows during their gaming experiences.</p>
]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;/assets/js/DPlayer.min.js&quot;&gt; &lt;/script&gt;&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;WarmWind&lt;/em&gt; is a 3D game developed with &lt;strong&gt;Unity 3D&lt;/stron
    
    </summary>
    
    
      <category term="Unity" scheme="http://www.weymire.com/tags/Unity/"/>
    
      <category term="Project" scheme="http://www.weymire.com/tags/Project/"/>
    
      <category term="Imagine Cup" scheme="http://www.weymire.com/tags/Imagine-Cup/"/>
    
  </entry>
  
  <entry>
    <title>森林的心</title>
    <link href="http://www.weymire.com/2014/08/02/HearOfForest/"/>
    <id>http://www.weymire.com/2014/08/02/HearOfForest/</id>
    <published>2014-08-02T11:15:00.000Z</published>
    <updated>2017-03-03T06:04:08.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><h2>Trailer</h2>
<p>由于年代久远, 游戏视频的原文件已经找不到, 只记得曾将其上传到56网上. 链接如下:</p>
<p><a href="http://www.56.com/u65/v_MTE5NDU5NTI2.html?refer2=direct" target="_blank" rel="external">森林的心</a></p>
<h2>Info</h2>
<p>这是我使用Unity引擎制作的第一款游戏. 高考后我开始自学Unity, 并参照其样例项目Stealth Project制作了<em>森林的心</em>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;/assets/js/DPlayer.min.js&quot;&gt; &lt;/script&gt;&lt;h2&gt;Trailer&lt;/h2&gt;
&lt;p&gt;由于年代久远, 游戏视频的原文件已经找不到, 只记得曾将其上传到56网上. 链接如下:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www
    
    </summary>
    
    
      <category term="Unity" scheme="http://www.weymire.com/tags/Unity/"/>
    
      <category term="Project" scheme="http://www.weymire.com/tags/Project/"/>
    
      <category term="Imagine Cup" scheme="http://www.weymire.com/tags/Imagine-Cup/"/>
    
      <category term="iOS/Android" scheme="http://www.weymire.com/tags/iOS-Android/"/>
    
  </entry>
  
</feed>
